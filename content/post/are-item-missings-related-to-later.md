---
title: 'Are item-missings related to later attrition?'
date: 2014-04-29T14:08:00.000+02:00
draft: false
tags : [missing data, measurement error, trade-off between survey errors, British Household Panel Study, attrition, responsive design, common causes of survey error, panel survey]
---

A follow up on [last month's post](https://www.peterlugtig.com/2014/03/do-respondents-become-sloppy-before.html). Respondents do seem to be less compliant in the waves before they drop out from a panel survey. This may however not neccesarily lead to worse data. So, what else do we see before attrition takes place? Let have a look at missing data:  
  
First, we look at missing data in a sensitive question on income amounts. Earlier studies ([here](https://www.jos.nu/Articles/abstract.asp?article=260145), [here,](https://www.jstor.org.proxy.library.uu.nl/stable/146438) [here](https://www.jstor.org.proxy.library.uu.nl/stable/1392158)) have already found that item nonresponse on sensitive questions predicts later attrition. I find that item nonresponse does increase before attrition, but only because of the fact that respondents are more likely to refuse to give an answer. And that increase is largely due to respondents who will later refuse to participate in the study as a whole. So, _item_ refusals are a good predictor of later _study_ refusals. The proportion of "Don't know" respondents does not increase over time.  
  

[![](https://1.bp.blogspot.com/-ZLkf9j9-qUk/U1-RxvTaZTI/AAAAAAAACqc/OqCXDLSAX1s/s1600/missings+PAYGL.png)](https://1.bp.blogspot.com/-ZLkf9j9-qUk/U1-RxvTaZTI/AAAAAAAACqc/OqCXDLSAX1s/s1600/missings+PAYGL.png)

_Missing income data in BHPS in 5 waves before attrition (click to enlarge)_

  
Does this finding for a sensitive question extend to all survey questions? No. Over all questions combined, I find that refusals  increase before attrition takes place, but  from a very low base (see the Y-axis scale in the figure below). Moreover, there is no difference between the groups, meaning that those who drop out of the survey do not have more item-missings than those respondents who are "always interviewed". It may seem odd that item missings increase for respondents who always happily participate. I suspect however that this may be related to the fact that both interviewers and respondents may have known in the last wave(s) [that the BHPS was coming to an end after 18 years of interviewing.](https://www.iser.essex.ac.uk/bhps)  

[  
](https://4.bp.blogspot.com/-S-ht4QK3lzQ/U1-TBeejkWI/AAAAAAAACqo/suuoODyJAik/s1600/dkplot.png)

  

[![](https://2.bp.blogspot.com/-LkHjq_AVszE/U1-TBdKdzbI/AAAAAAAACqk/0HLk_2un_0c/s1600/refuseplot.png)](https://2.bp.blogspot.com/-LkHjq_AVszE/U1-TBdKdzbI/AAAAAAAACqk/0HLk_2un_0c/s1600/refuseplot.png)

_Missing data for all survey questions in BHPS in waves before attrition (click to enlarge)_

What to do with this information? It seems that later study refusals can be identified using a combination of item nonresponses and survey compliance indicators. Once these respondents are identified, the next step would be to target them with survey design features that try to prevent attrition. These survey design features should target some of the concerns and motivations such respondents have that cause them to drop out from the survey.