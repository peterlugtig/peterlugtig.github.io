<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>attrition | Peter Lugtig</title>
    <link>https://thomvolker2.github.io/tags/attrition/</link>
      <atom:link href="https://thomvolker2.github.io/tags/attrition/index.xml" rel="self" type="application/rss+xml" />
    <description>attrition</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019 - 2020</copyright><lastBuildDate>Wed, 21 Jan 2015 15:21:00 +0100</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>attrition</title>
      <link>https://thomvolker2.github.io/tags/attrition/</link>
    </image>
    
    <item>
      <title>why panel surveys need to go &#39;adaptive&#39;</title>
      <link>https://thomvolker2.github.io/post/why-panel-surveys-need-to-go-adaptive/</link>
      <pubDate>Wed, 21 Jan 2015 15:21:00 +0100</pubDate>
      <guid>https://thomvolker2.github.io/post/why-panel-surveys-need-to-go-adaptive/</guid>
      <description>&lt;p&gt;Last week, I gave a talk at Statistics Netherlands (slides 
&lt;a href=&#34;https://www.dropbox.com/s/ul9msor9d6f9ak7/Lugtig%20-%20panel%20dropout%20%28talk%20at%20CBS%20January%202015%29.ppt.pdf?dl=0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
) about panel attrition. Initial and nonresponse and dropout from panel surveys have always been a problem. A famous study by Groves and Peytcheva (
&lt;a href=&#34;http://poq.oxfordjournals.org/content/70/5/646&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
) showed that in cross-sectional studies, nonresponse rates and nonresponse bias are only weakly correlated. In panel surveys however, all the signs are there that dropout in a panel study is often related to change. Those respondents undergoing most change, are also most likely to drop out. This is probably partly because of respondents (e.g. a move of house could be a good reason to change other things as well, like survey participation), but it is also because of how surveys deal with such moves. Movers are much harder to contact (if we don&#39;t have accurate contact details anymore). Movers are often assigned to a different interviewer. This will all lead to an underestimate of the number of people who move house in panel studies. Moving house is associated with lots of other life events (change in household composition, change in work, income etc.). In short dropout is a serious problem in longitudinal studies.&lt;/p&gt;
&lt;p&gt;The figure below shows the cumulative response rates for some large-scale panel studies. The selection of panel studies is a bit selective. I have tried to focus on large panel studies (so excluding cohort studies), which are still existing today, with a focus on Western Europe. &lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://3.bp.blogspot.com/-hQrr9XFijnI/VL-0qshxygI/AAAAAAAACts/lG8A-ZdO2Ho/s1600/Rplot08.tiff&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://3.bp.blogspot.com/-hQrr9XFijnI/VL-0qshxygI/AAAAAAAACts/lG8A-ZdO2Ho/s1600/Rplot08.tiff&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Cumulative nonresponse rates in large panel surveys (click to enlarge)&lt;/p&gt;
&lt;p&gt;The oldest study in the figure (PSID) has the highest initial response rate, followed by studies which were started in the 1980s (GSOEP), 1990s (BHPS), and early 2000s (HILDA). The more recent studies all have higher initial nonresponse rates. But not only that. They also have higher dropout rates (the lines go down much faster). This is problematic.&lt;/p&gt;
&lt;p&gt;I think these differences are not due to the fact that we, as survey methodologists, are doing a worse job now as compared to 20 years ago. If anything, we have been trying use more resources, professionalize tracking, offer higher incentives, and be more persistent. In my view, the increasing dropout rates are due to changes in society (the survey climate). A further increase of our efforts (e.g. higher incentives) could perhaps help somewhat to reduce future dropout. I think this is however not the way to go, especially as budgets for data collection face pressures everywhere.&lt;/p&gt;
&lt;p&gt;The way to reduce panel dropout is to collect data in a smarter way. First, we need to understand why people drop out. This is something we know quite well (but more can be done). For example, we know that likely movers are at risk. So, what we need are tailored strategies that focus on specific groups of people (e.g. likely movers). For example, we could send extra mailings in between waves only to them. We could use preventive tracking methods. We could put these into the field earlier.&lt;/p&gt;
&lt;p&gt;I am not the first to suggest such strategies. We have been tailoring our surveys for ages to specific groups, but have mostly done so at an ad-hoc basis,  never systematically. 
&lt;a href=&#34;http://jameswagnersurv.blogspot.nl/2010/09/responsive-design-and-adaptive-design.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Responsive or adaptive designs try to use tailoring systematically&lt;/a&gt;
, for those groups that most benefit from tailoring. Because we know so much about our respondents after wave 1, panel studies offer lots of opportunities to implement responsive designs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Are item-missings related to later attrition?</title>
      <link>https://thomvolker2.github.io/post/are-item-missings-related-to-later/</link>
      <pubDate>Tue, 29 Apr 2014 14:08:00 +0200</pubDate>
      <guid>https://thomvolker2.github.io/post/are-item-missings-related-to-later/</guid>
      <description>&lt;p&gt;A follow up on 
&lt;a href=&#34;http://www.peterlugtig.com/2014/03/do-respondents-become-sloppy-before.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;last month&#39;s post&lt;/a&gt;
. Respondents do seem to be less compliant in the waves before they drop out from a panel survey. This may however not neccesarily lead to worse data. So, what else do we see before attrition takes place? Let have a look at missing data:&lt;/p&gt;
&lt;p&gt;First, we look at missing data in a sensitive question on income amounts. Earlier studies (
&lt;a href=&#34;http://www.jos.nu/Articles/abstract.asp?article=260145&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
, 
&lt;a href=&#34;http://www.jstor.org.proxy.library.uu.nl/stable/146438&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here,&lt;/a&gt;
 
&lt;a href=&#34;http://www.jstor.org.proxy.library.uu.nl/stable/1392158&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
) have already found that item nonresponse on sensitive questions predicts later attrition. I find that item nonresponse does increase before attrition, but only because of the fact that respondents are more likely to refuse to give an answer. And that increase is largely due to respondents who will later refuse to participate in the study as a whole. So, &lt;em&gt;item&lt;/em&gt; refusals are a good predictor of later &lt;em&gt;study&lt;/em&gt; refusals. The proportion of &amp;ldquo;Don&#39;t know&amp;rdquo; respondents does not increase over time.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://1.bp.blogspot.com/-ZLkf9j9-qUk/U1-RxvTaZTI/AAAAAAAACqc/OqCXDLSAX1s/s1600/missings&amp;#43;PAYGL.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://1.bp.blogspot.com/-ZLkf9j9-qUk/U1-RxvTaZTI/AAAAAAAACqc/OqCXDLSAX1s/s1600/missings+PAYGL.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Missing income data in BHPS in 5 waves before attrition (click to enlarge)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Does this finding for a sensitive question extend to all survey questions? No. Over all questions combined, I find that refusals  increase before attrition takes place, but  from a very low base (see the Y-axis scale in the figure below). Moreover, there is no difference between the groups, meaning that those who drop out of the survey do not have more item-missings than those respondents who are &amp;ldquo;always interviewed&amp;rdquo;. It may seem odd that item missings increase for respondents who always happily participate. I suspect however that this may be related to the fact that both interviewers and respondents may have known in the last wave(s) 
&lt;a href=&#34;https://www.iser.essex.ac.uk/bhps&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;that the BHPS was coming to an end after 18 years of interviewing.&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;[&lt;br&gt;
](&lt;a href=&#34;http://4.bp.blogspot.com/-S-ht4QK3lzQ/U1-TBeejkWI/AAAAAAAACqo/suuoODyJAik/s1600/dkplot.png&#34;&gt;http://4.bp.blogspot.com/-S-ht4QK3lzQ/U1-TBeejkWI/AAAAAAAACqo/suuoODyJAik/s1600/dkplot.png&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://2.bp.blogspot.com/-LkHjq_AVszE/U1-TBdKdzbI/AAAAAAAACqk/0HLk_2un_0c/s1600/refuseplot.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://2.bp.blogspot.com/-LkHjq_AVszE/U1-TBdKdzbI/AAAAAAAACqk/0HLk_2un_0c/s1600/refuseplot.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Missing data for all survey questions in BHPS in waves before attrition (click to enlarge)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;What to do with this information? It seems that later study refusals can be identified using a combination of item nonresponses and survey compliance indicators. Once these respondents are identified, the next step would be to target them with survey design features that try to prevent attrition. These survey design features should target some of the concerns and motivations such respondents have that cause them to drop out from the survey.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Do respondents become sloppy before attrition?</title>
      <link>https://thomvolker2.github.io/post/do-respondents-become-sloppy-before/</link>
      <pubDate>Fri, 28 Mar 2014 15:33:00 +0100</pubDate>
      <guid>https://thomvolker2.github.io/post/do-respondents-become-sloppy-before/</guid>
      <description>&lt;p&gt;I am working on a paper that aims to link measurement errors to attrition error in a panel survey. For this, I am using the British Household Panel Survey. 
&lt;a href=&#34;http://www.peterlugtig.com/2013/11/longitudinal-interview-outcome-data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;In an earlier post&lt;/a&gt;
 I already argued that attrition can occur for many reasons, which I summarized in 5 categories.&lt;/p&gt;
&lt;p&gt;1. Noncontact&lt;br&gt;
2. Refusal&lt;br&gt;
3. Inability (due to old age, infirmity) as judged by the interviewer, also called &amp;lsquo;other non-interview&amp;rsquo;.&lt;br&gt;
4. Ineligibibility (due to death, or move into institution or abroad).&lt;br&gt;
5. people who were always interviewed&lt;/p&gt;
&lt;p&gt;In the paper, I study whether attrition due to any of the reasons above can be linked to increased measurement errors in the last waves before attrition. For example, earlier studies have found that item nonresponse to sensitive questions (income) predicts unit nonresponse in the next waves.&lt;/p&gt;
&lt;p&gt;For every respondent in the BHPS, I coded different indicators measurement error in every of the last five waves before attrition takes place. My working hypothesis is that measurement errors should increase in the last few waves before attrition takes place, due to decreasing respondent willingness and/or capability to participate.&lt;/p&gt;
&lt;p&gt;In the figure below, you find one set of indicators I used. Compliance to the survey does not count as an indicator of measurement error, but I found it interesting to look into nonetheless. I find that respondents are far less keen to do &amp;ldquo;extra&amp;rdquo; tasks in the waves before attrition. As measures, of compliance to these extra tasks, I looked at:&lt;/p&gt;
&lt;p&gt;1. the respondent cooperation as judged by the interviewer.&lt;br&gt;
2 the proportion of respondents who completes the tracking schedule at the end of the interview, and&lt;br&gt;
3. the proportion of respondents returning a self-completion questionnaire, left after the interview.&lt;/p&gt;
&lt;p&gt;In order to be able to interpret the results in a good way, I contrasted the 4 attrition groups with the 5th group of respondents who do not drop out, and are always interviewed.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://2.bp.blogspot.com/-L56iVULfRtk/UzWBGVFLSaI/AAAAAAAACqA/ceaAjOfnVfM/s1600/compliance.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://2.bp.blogspot.com/-L56iVULfRtk/UzWBGVFLSaI/AAAAAAAACqA/ceaAjOfnVfM/s1600/compliance.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Compliance with survey task by respondents in last 5 waves before attrition (click to enlarge)&lt;/p&gt;
&lt;p&gt;Unsuprisingly, I find that compliance decreases before attrition. Even at 5 waves before attrition, I find differences between the groups, with the &amp;ldquo;always interviewed&amp;rdquo; being most compliant, and the later to &amp;ldquo;refuse&amp;rdquo; group least compliant. The differences between the groups increase, the closer they get to attrition. Of the groups that attrite, the &amp;ldquo;noncontacts&amp;rdquo; and later &amp;ldquo;ineligibles&amp;rdquo; do only a little worse than the &amp;ldquo;always interviewed&amp;rdquo;. The &amp;ldquo;refusers&amp;rdquo; and &amp;ldquo;inables&amp;rdquo; have sharply decreasing cooperation ratings, and rates of completing the tracking schedule and returning the self-completion questionnaire. The differences between the groups are not large enough to predict exactly who is going to refuse or become unable to participate, but they can help to identify respondents being at risk.&lt;/p&gt;
&lt;p&gt;The next question would be what to do with this knowledge.  If a respondent really is unable to participate, there is not so much we as survey practitioners can do about this. Likely refusers may also be hard to target effectively. The rate of noncontacts is to a large degree under the control of survey practitioners, and for that reason, many nonresponse researchers are trying to limit noncontacts. Although refusers may be harder to target than noncontacts, it may be easier to identify &lt;em&gt;potential&lt;/em&gt; refusers, and take pre-emptive action, rather than use refusal conversion techniques after a  respondent has refused.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Personality predicts the likelihood and process of attrition in a panel survey</title>
      <link>https://thomvolker2.github.io/post/personality-predicts-likelihood-and/</link>
      <pubDate>Fri, 07 Feb 2014 17:12:00 +0100</pubDate>
      <guid>https://thomvolker2.github.io/post/personality-predicts-likelihood-and/</guid>
      <description>&lt;p&gt;Studies into the correlates of nonresponse often have to rely on socio-demographic variables to study whether respondents and nonrespondents in surveys differ. Often there is no other information available on sampling frames that researchers can use.&lt;/p&gt;
&lt;p&gt;That is unfortunate, for two reasons. First, the  variables we are currently using to predict nonrespons, usually explain a very limited amount of variance of survey nonresponse. Therefore, these variables are also not effective correctors for nonresponse. So, socio-demographic variables are not that interesting to have as sampling frame data. See my earlier posts 
&lt;a href=&#34;http://www.peterlugtig.com/2013/10/to-weight-or-to-impute-for-unit.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
 and 
&lt;a href=&#34;http://www.peterlugtig.com/2013/10/imagine-we-have-great-covariates-for.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;It is also unfortunate, because theoretically, we can come up with other respondent characteristics that should explain nonresponse much better. For example, whether respondents believe surveys to be important, whether they enjoy thinking (need for cognition) and whether they have a conscientious personality.&lt;/p&gt;
&lt;p&gt;I published a new paper today in Sociological Methods and Research, that links these variables in particular to different patterns of attrition in a longitudinal survey. See the full paper 
&lt;a href=&#34;http://smr.sagepub.com/content/early/2014/02/06/0049124113520305.abstract&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Among the 2007 sample members of the Dutch 
&lt;a href=&#34;http://www.lissdata.nl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LISS Panel&lt;/a&gt;
, I tested for different drop out patterns, and classified respondents according to their attrition process. Some respondents are continuous respondents (stayers - on top), while others start enthusiastically, but drop out at various stages of the survey (lines going down), or never really enthusiastically participate (lurkers - erratic lines in the middle).&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://4.bp.blogspot.com/-n_UBRgc5b2s/UvUBfNfR5XI/AAAAAAAACpY/ayRtFnVLhSo/s1600/LCA&amp;#43;9&amp;#43;SMR.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://4.bp.blogspot.com/-n_UBRgc5b2s/UvUBfNfR5XI/AAAAAAAACpY/ayRtFnVLhSo/s1600/LCA+9+SMR.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Figure 1: attrition patterns among original LISS sample members (click to enlarge)&lt;/p&gt;
&lt;p&gt;As a next step, I linked the attrition classes to a set of predictors. Among them socio-demographic variables, but also psychological, and survey related ones.&lt;/p&gt;
&lt;p&gt;The 3 strongest predictors of the differences between the attrition patterns (latent classes) are:&lt;br&gt;
- Personality. Conscientious and less extravert people drop out less often&lt;br&gt;
- Survey enjoyment. If people enjoy completing surveys, they are not likely to drop out.&lt;br&gt;
- Having received a PC. The LISS panel gave respondents without Internet access a computer and broadband Internet to enable them to participate. Almost none of the people who received such a computer dropped out from the study.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://www.peterlugtig.com/2012/09/is-panel-attrition-same-as-nonresponse.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;In an earlier post&lt;/a&gt;
 about an upcoming book chapter, I already noted that the correlates of attrition and initial nonresponse are very different in the LISS, so personality may not explain initial nonresponse in other panel surveys or nonresponse in cross-sectional surveys.&lt;/p&gt;
&lt;p&gt;It would be very hard to test my hypothesis that personality is a strong predictor of all types of nonresponse. Unless you would do a survey among employees, and have simultaneous access all personality data from tests that every job applicant at that company took. If you sit on top such data and want me to analyse them, do e-mail me.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Longitudinal interview outcome data reduction: Latent Class and Sequence analyses</title>
      <link>https://thomvolker2.github.io/post/longitudinal-interview-outcome-data/</link>
      <pubDate>Mon, 04 Nov 2013 16:22:00 +0100</pubDate>
      <guid>https://thomvolker2.github.io/post/longitudinal-interview-outcome-data/</guid>
      <description>&lt;p&gt;Frauke Kreuter once commented on a presentation I gave that I should really be looking at sequence analysis for studying attrition in panel surveys. She had written an article on the topic with Ulrich Kohler (
&lt;a href=&#34;http://www.jos.nu/Articles/abstract.asp?article=252203&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
) in 2009, and as of late there are more people exploring the technique (e.g. Mark Hanly at Bristol, and 
&lt;a href=&#34;http://www.southampton.ac.uk/demography/about/staff/gbd.page&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gabi Durrant at Southampton&lt;/a&gt;
).&lt;/p&gt;
&lt;p&gt;I am working on a project on attrition in the British Household Panel, and linking attrition errors to measurement errors. Attrition data can be messy. Below, you see the response outcome sequences of every initial panel member in the British Household Panel Survey. This figure obscures the fact individual respondents may frequently switch states (e.g. interview - noncontact - interview - refusal - not issued).&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://4.bp.blogspot.com/-JQnC4Q1FXFw/Une6AMXsrHI/AAAAAAAACn0/sfGgWexzVlA/s1600/Plot&amp;#43;all&amp;#43;sequences&amp;#43;BHPS.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://4.bp.blogspot.com/-JQnC4Q1FXFw/Une6AMXsrHI/AAAAAAAACn0/sfGgWexzVlA/s400/Plot+all+sequences+BHPS.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1:&lt;/em&gt; &lt;em&gt;relative sizes of final interview outcomes at 18 waves of BHPS of wave 1 respondents&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Although descriptive visualizations like these are informative, sequence analysis becomes analytically interesting when you try to &amp;ldquo;do&amp;rdquo; something with the sequences of information. In my case, I want to group all sequence chains into &amp;ldquo;clusters&amp;rdquo; or &amp;ldquo;classes&amp;rdquo; of people who have a similar process of attrition. 
&lt;a href=&#34;http://www.stata-journal.com/article.html?article=st0111&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stata&lt;/a&gt;
 and 
&lt;a href=&#34;http://mephisto.unige.ch/traminer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R TraMineR&lt;/a&gt;
 offer possibilities for doing this. Both packages enable you to match sequences by 
&lt;a href=&#34;http://en.wikipedia.org/wiki/Optimal_matching&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;optimal matching&lt;/a&gt;
, so that for every sequence from every person you get a distance measure to every other sequence (person). In turn, this (huge) dustance matrix can then be used to classify all the sequences of all respondents into clusters. R offers a nice way to handle huge data matrices, by using aggregation and weighting by the way. See the 
&lt;a href=&#34;http://mephisto.unige.ch/weightedcluster/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WeightedCluster&lt;/a&gt;
 library.&lt;/p&gt;
&lt;p&gt;Below, you find the results of the sequence analysis. The nice thing is that I end up with 6 clusters that look the same as the Classes that I got out of a Latent Class Analysis over summer. So now I feel much more confident using this classification.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://4.bp.blogspot.com/-5-Z64s2GcV4/Une6APMFHsI/AAAAAAAACoA/JQ-QJeO7S0A/s1600/6-cluster&amp;#43;solution&amp;#43;from&amp;#43;sequence&amp;#43;analysis&amp;#43;on&amp;#43;BHPS&amp;#43;attrition&amp;#43;data.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://4.bp.blogspot.com/-5-Z64s2GcV4/Une6APMFHsI/AAAAAAAACoA/JQ-QJeO7S0A/s640/6-cluster+solution+from+sequence+analysis+on+BHPS+attrition+data.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 2: 6-cluster solution for sequence analysis on BHPS attrition patterns&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The differences between sequence analysis and LCA are really minimal, and probably result from the fact that the Optimal Matching algorithm used in sequence analysis is more flexible (in that it would allow deletion, additions, substitutions etc to match), than Latent Class Analysis. But in practice, for my analyses, it doesn&#39;t matter what technique I use. My results are equivalent. Personally, I like Latent Class analysis more, because it offers the option of linking the Latent Classes of attrition to substantive research data in one model.&lt;/p&gt;
&lt;p&gt;Attrition data bear a resemblence to contact data recorded in a telephone or face-to-face survey. Interviewers make call or interview attempts, that bear a lot of information about the survey proecess, and could improve fieldwork, and reduce nonresponse and costs. I am imagining a paper where one links contact data at every wave, and combines that with attrition analyses, in a series of linked-sequence data analysis. That way, you can learn how specific sequences of call and contact data, lead to specific sequences of interview outcomes at later stages of the panel survey. It can be done if you have a lot of time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Imagine we have great covariates for correcting for unit nonresponse...</title>
      <link>https://thomvolker2.github.io/post/imagine-we-have-great-covariates-for/</link>
      <pubDate>Mon, 14 Oct 2013 10:41:00 +0200</pubDate>
      <guid>https://thomvolker2.github.io/post/imagine-we-have-great-covariates-for/</guid>
      <description>&lt;p&gt;I am continuing on the recent article and commentaries on weighting to correct for unit nonresponse by Michael Brick, as published in the recent issue of the Journal of Official Statistics (
&lt;a href=&#34;http://www.jos.nu/Articles/abstract.asp?article=293329&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
).&lt;/p&gt;
&lt;p&gt;The article by no means is all about whether one should impute or weight. I am just picking out one issue that got me thinking. Michael Brick rightly says that in order to correct succesfully for unit nonresponse using covariates, we want the covariates to do two things:&lt;/p&gt;
&lt;p&gt;1. They should explain missingness.&lt;br&gt;
2. They should highly correlate with our variable of interest.&lt;/p&gt;
&lt;p&gt;In other words, these are the two assumptions for a  Missing At Random process of missing data.&lt;/p&gt;
&lt;p&gt;The variables (covariates) we currently use for nonresponse adjustments do neither. Gender, age, ethnicity, region, (and if we&#39;re lucky) education, household composition and house characterics do not explain missingness, nor our variable of interest. Would it ever be conceivable to obtain covariates that do this? What are the candidates?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. covariates (X) that explain missingness (R):&lt;/strong&gt;&lt;br&gt;
Paradata are currently our best bet. Those may be interviewer observations or call data during fieldwork (note the absence of sample level paradata for self-administered surveys - here lies a task for us). Paradata don&#39;t explain missingness very well at the moment, but I think everyone in survey research agrees we can try to collect more.&lt;br&gt;
Another set of candidates are variables that we obtain by enriching sampling frames. We can use marketing data, social networks, or census data to get more information on our sampling units.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. covariates (X) that explain our variable of interest (Y)&lt;/strong&gt;:&lt;br&gt;
Even if we find covariates that explain missingness, we also want those covariates to be highly correlated to our variable of interest. It is very unlikely that a fixed set of for example paradata variables can ever achieve that. Enriched frame data may be more promising, but is unlikely that this will generally work. I think it is a huge problem that our nonresponse adjustment variables (X) are not related to Y, and one that is not likely to ever be resolved for cross-sectional surveys.&lt;/p&gt;
&lt;p&gt;But. In longitudinal surveys, this is an entirely different matter. Because we usually ask the same variables over time, we can use variables from earlier occasions to predict values that are missing at later waves. So, there, we have great covariates that explain our variable of interest. We can use those as long as MAR holds. If change in the dependent variable is associated with attrition, MAR does not hold. Strangely, I know very few studies that study whether attrition is related to change in the dependent variable. Usually, attrition studies focus on covariates measured before attrition, to then explain attrition. They do not focus on change in the dependent variable.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://2.bp.blogspot.com/-NhM3-D53n-g/UlutJ3JdvgI/AAAAAAAACnY/vmiOUDyFvdk/s1600/missing&amp;#43;data&amp;#43;mechanisms.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://2.bp.blogspot.com/-NhM3-D53n-g/UlutJ3JdvgI/AAAAAAAACnY/vmiOUDyFvdk/s400/missing+data+mechanisms.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Covariate adjustment for nonresponse in cross-sectional and longitudinal surveys&lt;/p&gt;
&lt;p&gt;(follow-up 28 October 2013): When adjustment variables are strongly linked to dependent variables, but not to nonresponse, variances tend to be increased (See Little and Vartivarian). So, in longitudinal surveys, the weak link between X and R should really be of medium strength as well, if adjustment is to be successful.&lt;/p&gt;
&lt;p&gt;I once thought that because we have so much more information in longitudinal surveys, we could use the lessons that we learn from attrition analyses to improve nonresponse adjustments in cross-sectional surveys. In a 
&lt;a href=&#34;http://www.peterlugtig.com/2012/09/is-panel-attrition-same-as-nonresponse.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;forthcoming book chapter&lt;/a&gt;
, I found that the correlates of attrition are however very different from the correlates of nonresponse in wave 1. So in my view, the best we can do in cross-sectional surveys is to focus on explaining missingness, and then hope for the best for the prediction of our variables of interest.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>measurement and nonresponse error in panel surveys</title>
      <link>https://thomvolker2.github.io/post/measurement-and-nonresponse-error-in/</link>
      <pubDate>Thu, 04 Jul 2013 11:57:00 +0200</pubDate>
      <guid>https://thomvolker2.github.io/post/measurement-and-nonresponse-error-in/</guid>
      <description>&lt;p&gt;I am spending time at the Institute for Social and Economic Research in Colchester, UK where I will work on a research project that investigates whether there is a tradeoff between nonresponse and measurement errors in panel surveys.&lt;/p&gt;
&lt;p&gt;Survey methodologists have long believed that multiple survey errors have a common cause. For example, when a respondent is less motivated, this may result in nonresponse (in a panel study attrition), or in reduced cognitive effort during the interview, which in turn leads to measurement errors. Lower cognitive abilities and language problems might be other examples of common caused that lead to either nonresponse or measurement error. Understanding these common error sources is important to know whether our efforts to reduce 1 survey error source are not offset by an increase in another one. It follows from the idea that good survey design minimize 
&lt;a href=&#34;http://poq.oxfordjournals.org/content/74/5.toc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Total Survey Error&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Studying the trade-off has proven to be very difficult. This is because nonrespondents are by definition not observed. So, we never know how nonrespondents would answer questions, and how much measurement error is included in those answers. We can only observe measurement errors for respondents, but can not compare these to the potential measurement error of nonrespondents.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;data:image/png;base64,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&#34; alt=&#34;&#34;&gt;&lt;br&gt;
&lt;em&gt;Hypothetical continuum of timing of survey response&lt;/em&gt;&lt;br&gt;
[](&lt;a href=&#34;http://www.blogger.com/blogger.g?blogID=7827313755221690631&#34;&gt;http://www.blogger.com/blogger.g?blogID=7827313755221690631&lt;/a&gt;)&lt;br&gt;
To overcome this problem, most methodologists have compared &amp;lsquo;early&amp;rsquo; respondents (people who respond very quickly in the fieldwork period) to &amp;lsquo;late&amp;rsquo; respondents (those who only participate after being reminded for example). The idea behind this, is that the probability of response is:&lt;/p&gt;
&lt;p&gt;a) a linear continuum from very early response on the one extreme, and nonresponse on the other.&lt;br&gt;
b) that hypothetically, nonrespondents could be converted into respondents if extreme amounts of efforts are used to do so (Voogt 2005 showed in a small-scale study in the Dutch locality of Zaandam that this is actually possible)&lt;/p&gt;
&lt;p&gt;So, the idea in summary is that late respondents can serve as a proxy for information about nonrespondents. However, that assumption is not likely to be true in general, if ever.&lt;/p&gt;
&lt;p&gt;In my project, I will try to overcome this problem, that we never have measurement error estimates for nonrespondents. I use longitudinal data and Structural Equation Modeling techniques to estimate measurement errors for nonrespondents in the British Household Panel Study, compare them to respondents, and link them to potential common causes of both type of errors. See 
&lt;a href=&#34;https://dl.dropboxusercontent.com/u/2839696/Lugtig%20-%20Jess%20semninar%2019%20june%202013.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this presentation&lt;/a&gt;
 for more details on this project&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Is panel attrition the same as nonresponse?</title>
      <link>https://thomvolker2.github.io/post/is-panel-attrition-same-as-nonresponse/</link>
      <pubDate>Sat, 29 Sep 2012 10:49:00 +0200</pubDate>
      <guid>https://thomvolker2.github.io/post/is-panel-attrition-same-as-nonresponse/</guid>
      <description>&lt;p&gt;All of my research is focused on the methods of assembling and analysis of panel survey data. One of the primary problems of panel survey projects is attrition or drop-out. Over the course of a panel survey, many respondents decide to no longer participate.&lt;/p&gt;
&lt;p&gt;Last july I visited the panel survey methods workshop in Melbourne, at which we had extensive discussions about panel attrition. How to study it, what the consequences are (bias) for survey estimates, and how to prevent it from happening altogether.&lt;/p&gt;
&lt;p&gt;These questions have a lot in common with the questions that are being discussed at another workshop for survey methodologists: the nonresponse workshop. The only difference is that at the nonresponse workshop we discuss one-off, cross-sectional surveys, and at the panel survey workshop, we dicuss what happens after the first wave of data collection.&lt;/p&gt;
&lt;p&gt;I am in the middle of writing a book chapter (with Annette Scherpenzeel and Marcel Das of Centerdata) on attrition in the LISS Internet panel, and one of the questions that we try to answers is whether nonrespondents in the first wave are actually similar to respondents who drop out at wave 2 or later. Or to be more precise, whether nonrespondents are actually similar to fast attriters, or to other sub-groups of attriters.&lt;/p&gt;
&lt;p&gt;The graph below shows attrition patterns for the people in the LISS panel for the 50 waves that we analysed. The green line on top represents people who have response propensities close to 1, meaning they always participate. The brown line represents fast attriters, and the pink, dark blue, and purple lines slowers groups that drop out more slowly. You also find new panel entrants (dark grey and red line), and finally, a almost invisible black line that has response propensities of 0, meaning that although these people consented to become a panel member, they never actually participate in the panel.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://2.bp.blogspot.com/-bQM6mKwQHSQ/UGa00GA-Y0I/AAAAAAAACfM/nuzk9xKEmC0/s1600/graph&amp;#43;lca9.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://2.bp.blogspot.com/-bQM6mKwQHSQ/UGa00GA-Y0I/AAAAAAAACfM/nuzk9xKEmC0/s400/graph+lca9.jpg&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;click on the Figure to enlarge&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For the whole story you&#39;ll have to wait for book on &amp;lsquo;Internet panel surveys&amp;rsquo; to come out somewhere in 2013, but I&#39;ll focus here on comparing initial nonrespondents to respondents who do consent, but then never participate.&lt;br&gt;
These groups turn out to be different. Not just a little different, but hugely different. This was somewhat surprising to me, as many survey methodologists believe that early panel attrition is some kind of continuation of initial nonresponse. It turns out not to be. Fast attriters are very different from initial nonrespondents. My hypothesis for this is that some specific groups of people may &#39; accidentily&amp;rsquo;  say yes to a first survey request, but then try to get out of the survey as fast as they can. I am still not sure what this implies for panel research (comments very welcome): does it mean that the methods that we use to target nonrespondents (persuasion principles of Cialdini et al. 1991) might not work in panel surveys, and that we need to use different methods?&lt;/p&gt;
&lt;p&gt;I think the first few waves of a panel study are extremely important for keeping attrition low in the long run. So, I think we should perhaps prolong some of the efforts that we use in the recruitment phase (advance letters, mixed-mode contact strategy), in the first waves as well, only to resort to a cheaper contact mode later, once panel members have developed a habit of responding to the different waves in a panel.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
