<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>opinion poll | Peter Lugtig</title>
    <link>/tags/opinion-poll/</link>
      <atom:link href="/tags/opinion-poll/index.xml" rel="self" type="application/rss+xml" />
    <description>opinion poll</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019 - 2020</copyright><lastBuildDate>Sat, 22 Sep 2012 16:05:00 +0200</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>opinion poll</title>
      <link>/tags/opinion-poll/</link>
    </image>
    
    <item>
      <title>why access panels cannot weight elections polls accurately</title>
      <link>/post/why-access-panels-cannot-weight/</link>
      <pubDate>Sat, 22 Sep 2012 16:05:00 +0200</pubDate>
      <guid>/post/why-access-panels-cannot-weight/</guid>
      <description>&lt;p&gt;There are a lot of reasons why would not want to use acces panels for predicting electoral outcomes . These are well discussed in many places on- and offline. I&amp;rsquo;ll shortly summarize them, before adding some thoughts to why access panels do so badly predicting election outcomes.&lt;/p&gt;
&lt;p&gt;1. Access panels don&amp;rsquo;t draw random samples, but rely on self-selected samples. A slightly better way to get panel respondents is a quota sample, but even these have problems, well discussed here, here and here for example. The bottom line is that access panel respondents are not &#39; normal&amp;rsquo; people, and so voting preferences of not-normal people are likely to be biased.&lt;/p&gt;
&lt;p&gt;2. Because of these problems, survey managers use weighting. They correct their sample for known biases in the sample. If they know elderly people with low educations are underrepresented in an access panel, they weigh them up. I think this is bad practice. And it has been shown that weighting does not solve the problem,. and can sometimes make biases worse for general surveys. Here are some additional and specific problems, often neglected. In short, weighting only works if the weighting variables can predict the dependent variable to a great extent.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://1.bp.blogspot.com/-vjBRTegZb3Q/UF3EyedoEQI/AAAAAAAACe8/lc7kLoJxNis/s1600/Unbalanced_scales-too-far-right.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://1.bp.blogspot.com/-vjBRTegZb3Q/UF3EyedoEQI/AAAAAAAACe8/lc7kLoJxNis/s200/Unbalanced_scales-too-far-right.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Weighting is usually done with socio-demographic variables. From political science research, we know that sociodemographics do a bad job of explaining voting behavior. Explained variances for regression models normally don&amp;rsquo;t exceed 10%.&lt;br&gt;
So, let me get down to the main point I would like to make in this post. A point which I have not seen discussed anywhere.&lt;/p&gt;
&lt;p&gt;Panel survey managers have &amp;lsquo;resolved&amp;rsquo; the weakness of their weighting models by including a variable that does predict voring behavior fairly well: past voting behavior. If one knows that past Social Democrat voters are underrepresented, one can weight on that variable. This is all very well, if one has good data of past voting behavior for all panel members. The panels currently do not. Their information is wrong in two ways:&lt;/p&gt;
&lt;p&gt;1. Access panels will never have information for people &lt;strong&gt;who did not vote previously&lt;/strong&gt;. These are mainly young people, or people who normally do not vote in elections. If these new voters vote like everyone else there is no problem, but new voters have very specific voting preferences.&lt;/p&gt;
&lt;p&gt;2.  Reversely, access panels can not predict well &lt;strong&gt;who is not going to vote in current elections&lt;/strong&gt;. If non-voters disproportionally voted for one party in the previous elections, this will lead to an overestimation of voters for that party.&lt;/p&gt;
&lt;p&gt;I believe these two problems are larger than most people think. The first problem can predict why the PVV-vote was underestimated in 2006 and 2010. The PVV attracted many new voters in those elections. The second problem explains why the PVV-vote was overestimated in 2012.  Many people who voted PVV in the previous elections, stayed home this time.&lt;/p&gt;
&lt;p&gt;So, panel survey managers who want a bit of free advice how to improve your polls. Try to get a clear view on the new voters, and the people unlikely to vote. That may be hard, especially because non-voters are not so interested in politics, and will therefore not sign up for online access panels voluntarily. But it is certainly not impossible.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dutch elections 2012 - poll results</title>
      <link>/post/dutch-elections-2012-poll-results/</link>
      <pubDate>Thu, 13 Sep 2012 11:57:00 +0200</pubDate>
      <guid>/post/dutch-elections-2012-poll-results/</guid>
      <description>&lt;p&gt;The night after the election, one can conclude that all pollsters in the Netherlands did a bad job of predicting the election results. All polls were at least off by 20 seats (out of 150), and I expect the newspapers to make headlines of this in the next days. See the table below for the final predictions (before election day), the exit poll and final election results. The last row shows how much each poll was off (in the number of seats&lt;/p&gt;
&lt;p&gt;Actually, I think the pollsters did pretty well this time. The only thing all of them mispredicted, was a large number of PVV voters moving to VVD, and a lot of SP voters moving to the PvdA, This movement was visible in the last polls leading up to the elections, but the pollsters either underestimated it, or a lot of people switched for the winner on election date.&lt;/p&gt;
&lt;p&gt;So, I predicted Synovate would do best, but that did not turn out to be the case. Well, they share first place with Maurice de Hond, but are not clearly better than others. There are lots of blogs, articles and news items about Internet Panels these days. I spent some blogposts on that issue in 2009 myself. Although the largest reason why pollers generally do so badly is that they do not draw random samples, I think there are two more reasons why pollsters do badly. I plan to spend my next two blog posts on these topics, so stay tuned for more on the following issue.&lt;/p&gt;
&lt;p&gt;Pollsters use statistical weighting to account for the unrepresentativity of their panel. They do this on sociodemographic characteristics and past voting behavior. I believe it is wrong to weight (in general), and specifically to do so on past voting behavior. I&amp;rsquo;ll show you why in the next days.&lt;/p&gt;
&lt;p&gt;No. of seats in partliament 2012&lt;/p&gt;
&lt;p&gt; Maurice de Hond (peil.nl)&lt;/p&gt;
&lt;p&gt;Intomart/de stemming&lt;/p&gt;
&lt;p&gt; Synovate&lt;/p&gt;
&lt;p&gt; TNS-NIPO&lt;/p&gt;
&lt;p&gt;Exit Poll (synovate)&lt;/p&gt;
&lt;p&gt;Final results&lt;/p&gt;
&lt;p&gt;VVD (right-liberal)&lt;/p&gt;
&lt;p&gt;36&lt;/p&gt;
&lt;p&gt;35&lt;/p&gt;
&lt;p&gt;37&lt;/p&gt;
&lt;p&gt;35&lt;/p&gt;
&lt;p&gt;41&lt;/p&gt;
&lt;p&gt;41&lt;/p&gt;
&lt;p&gt;PVDA (social democrat)&lt;/p&gt;
&lt;p&gt;36&lt;/p&gt;
&lt;p&gt;34&lt;/p&gt;
&lt;p&gt;36&lt;/p&gt;
&lt;p&gt;34&lt;/p&gt;
&lt;p&gt;40&lt;/p&gt;
&lt;p&gt;38&lt;/p&gt;
&lt;p&gt;SP (socialist)&lt;/p&gt;
&lt;p&gt;20&lt;/p&gt;
&lt;p&gt;22&lt;/p&gt;
&lt;p&gt;21&lt;/p&gt;
&lt;p&gt;21&lt;/p&gt;
&lt;p&gt;15&lt;/p&gt;
&lt;p&gt;15&lt;/p&gt;
&lt;p&gt;PVV (anti-immigrant)&lt;/p&gt;
&lt;p&gt;18&lt;/p&gt;
&lt;p&gt;17&lt;/p&gt;
&lt;p&gt;17&lt;/p&gt;
&lt;p&gt;17&lt;/p&gt;
&lt;p&gt;13&lt;/p&gt;
&lt;p&gt;15&lt;/p&gt;
&lt;p&gt;CDA (christian democrats)&lt;/p&gt;
&lt;p&gt;12&lt;/p&gt;
&lt;p&gt;12&lt;/p&gt;
&lt;p&gt;13&lt;/p&gt;
&lt;p&gt;12&lt;/p&gt;
&lt;p&gt;13&lt;/p&gt;
&lt;p&gt;13&lt;/p&gt;
&lt;p&gt;D&amp;rsquo;66 (center liberals)&lt;/p&gt;
&lt;p&gt;11&lt;/p&gt;
&lt;p&gt;11&lt;/p&gt;
&lt;p&gt;10&lt;/p&gt;
&lt;p&gt;13&lt;/p&gt;
&lt;p&gt;12&lt;/p&gt;
&lt;p&gt;12&lt;/p&gt;
&lt;p&gt;CU (christian union)&lt;/p&gt;
&lt;p&gt;5&lt;/p&gt;
&lt;p&gt; 7&lt;/p&gt;
&lt;p&gt;5&lt;/p&gt;
&lt;p&gt;6&lt;/p&gt;
&lt;p&gt;4&lt;/p&gt;
&lt;p&gt;5&lt;/p&gt;
&lt;p&gt;SGP (reformed christians)&lt;/p&gt;
&lt;p&gt;3&lt;/p&gt;
&lt;p&gt;3&lt;/p&gt;
&lt;p&gt;2&lt;/p&gt;
&lt;p&gt;2&lt;/p&gt;
&lt;p&gt;3&lt;/p&gt;
&lt;p&gt;3&lt;/p&gt;
&lt;p&gt;Groenlinks (green)&lt;/p&gt;
&lt;p&gt;4&lt;/p&gt;
&lt;p&gt;4&lt;/p&gt;
&lt;p&gt;4&lt;/p&gt;
&lt;p&gt; 4&lt;/p&gt;
&lt;p&gt;4&lt;/p&gt;
&lt;p&gt;4&lt;/p&gt;
&lt;p&gt;PVDD (animal rights&lt;/p&gt;
&lt;p&gt;3&lt;/p&gt;
&lt;p&gt;2&lt;/p&gt;
&lt;p&gt;3&lt;/p&gt;
&lt;p&gt; 2&lt;/p&gt;
&lt;p&gt;2&lt;/p&gt;
&lt;p&gt;2&lt;/p&gt;
&lt;p&gt;50plus (elderly)&lt;/p&gt;
&lt;p&gt;2&lt;/p&gt;
&lt;p&gt;3&lt;/p&gt;
&lt;p&gt; 2&lt;/p&gt;
&lt;p&gt; 4&lt;/p&gt;
&lt;p&gt;3&lt;/p&gt;
&lt;p&gt;2&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Wrongly predicted&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;18&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;24&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;18&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;24&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Electoral volatility due to different questions?</title>
      <link>/post/electoral-volatility-due-to-different/</link>
      <pubDate>Fri, 24 Aug 2012 13:15:00 +0200</pubDate>
      <guid>/post/electoral-volatility-due-to-different/</guid>
      <description>&lt;p&gt;**Poll volatility **&lt;/p&gt;
&lt;p&gt;Below, you find the summed changes in parliamentary seats over all parties in consecutive opinion polls for the four main polling firms in the Netherlands in the lead up to the 2012 elections. I will update the table below in the next weeks.&lt;br&gt;
This overview follows from my earlier post on Dependent Interviewing. Maurice de Hond (peil.nl) is the only survey pre-loading earlier voter preferences into survey questions. I expect this to lead to less volatility in voter preferences for Maurice de Hond, as compared to the other polling firms.&lt;/p&gt;
&lt;p&gt;Update September 12th: With the final polls out on election day, it seems that the polls of Maurice de Hond are indeed most stable over time, and in my previous post I argues this was because of the fact that he uses Dependent Interviewing in his question on &amp;quot; what would you vote if there were elections today&amp;rdquo;. Still, I would have expected a larger effect. Let&amp;rsquo;s see tomorrow which polling firm did best. My bet: Synovate, because they are using the most sound (although still not perfect) methodology of polling people. More on that tomorrow&amp;hellip;&lt;/p&gt;
&lt;p&gt; Maurice de Hond (peil.nl)&lt;/p&gt;
&lt;p&gt;Intomart/de stemming&lt;/p&gt;
&lt;p&gt; Synovate&lt;/p&gt;
&lt;p&gt; TNS-NIPO&lt;/p&gt;
&lt;p&gt; week 23 (03-06)&lt;/p&gt;
&lt;p&gt;4&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;p&gt; 8&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;p&gt; 10-06&lt;/p&gt;
&lt;p&gt;8&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;p&gt; 17-06&lt;/p&gt;
&lt;p&gt;6&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;p&gt; 8*&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;p&gt; 24-06&lt;/p&gt;
&lt;p&gt;4&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;p&gt; 14**&lt;/p&gt;
&lt;p&gt; 01-07&lt;/p&gt;
&lt;p&gt;10&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;p&gt; 12*&lt;/p&gt;
&lt;p&gt; 8&lt;/p&gt;
&lt;p&gt; 08-07&lt;/p&gt;
&lt;p&gt;12&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;p&gt; 12&lt;/p&gt;
&lt;p&gt; 12&lt;/p&gt;
&lt;p&gt; 15-07&lt;/p&gt;
&lt;p&gt;6&lt;/p&gt;
&lt;p&gt; 8&lt;/p&gt;
&lt;p&gt; 10&lt;/p&gt;
&lt;p&gt; 12&lt;/p&gt;
&lt;p&gt; 22-07&lt;/p&gt;
&lt;p&gt;2&lt;/p&gt;
&lt;p&gt; 10&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;p&gt; 6&lt;/p&gt;
&lt;p&gt; 29-07&lt;/p&gt;
&lt;p&gt;4&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;p&gt; 8*&lt;/p&gt;
&lt;p&gt; 6&lt;/p&gt;
&lt;p&gt; 05-08&lt;/p&gt;
&lt;p&gt;2&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;p&gt; 10&lt;/p&gt;
&lt;p&gt; 12-08&lt;/p&gt;
&lt;p&gt;10&lt;/p&gt;
&lt;p&gt;10**&lt;/p&gt;
&lt;p&gt; 8*&lt;/p&gt;
&lt;p&gt; 16&lt;/p&gt;
&lt;p&gt; 19-08&lt;/p&gt;
&lt;p&gt;8&lt;/p&gt;
&lt;p&gt;  8&lt;/p&gt;
&lt;p&gt;12&lt;/p&gt;
&lt;p&gt;  8&lt;/p&gt;
&lt;p&gt; 26-08&lt;/p&gt;
&lt;p&gt;6&lt;/p&gt;
&lt;p&gt;20&lt;/p&gt;
&lt;p&gt;8&lt;/p&gt;
&lt;p&gt;16&lt;/p&gt;
&lt;p&gt;03-09&lt;/p&gt;
&lt;p&gt;22&lt;/p&gt;
&lt;p&gt;20&lt;/p&gt;
&lt;p&gt;12&lt;/p&gt;
&lt;p&gt;14&lt;/p&gt;
&lt;p&gt;10-09&lt;/p&gt;
&lt;p&gt;26&lt;/p&gt;
&lt;p&gt;10&lt;/p&gt;
&lt;p&gt;16&lt;/p&gt;
&lt;p&gt;20&lt;/p&gt;
&lt;p&gt; average change&lt;/p&gt;
&lt;p&gt;8,7&lt;/p&gt;
&lt;p&gt;12,2***&lt;/p&gt;
&lt;p&gt;10,3***&lt;/p&gt;
&lt;p&gt;11,8&lt;/p&gt;
&lt;p&gt;* two-week difference&lt;br&gt;
** three-week difference&lt;br&gt;
*** rounded down due to inclusion of multi-week changes&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>panel conditioning</title>
      <link>/post/panel-conditioning/</link>
      <pubDate>Thu, 12 Jan 2012 14:12:00 +0100</pubDate>
      <guid>/post/panel-conditioning/</guid>
      <description>&lt;p&gt;In late august of 2011 I attended the Internet Survey Methodology Workshop. There were people from academia, official statistics and market research agencies there. One of the issues discussed there has had me thinking since: the topic of panel conditioning. Some people seem really worried that respondents in panel surveys start behaving or thinking differently because of repeated participation in a survey.&lt;/p&gt;
&lt;p&gt;Panel conditioning is closely linked with the issue of  &amp;lsquo;professional&amp;rsquo; respondents. These are respondents who know exactly how survey researchers design surveys, and use this knowledge to get most out of the survey (in terms of reward-schemes) against the least time possible.&lt;/p&gt;
&lt;p&gt;Many market research firms throw out respondents after some time, mostly a couple of years, and then refresh their samples. But is this necessary? If so, after what time do respondents become conditioned? And for what topics is conditioning most problematic?&lt;/p&gt;
&lt;p&gt;Several studies from the 1970s focused on voting behavior in election panel studies. They found that respondents who were asked before a general election aboyut their voting behavior were 10-15% more likely to vote than respondents who were only asked about their voting behavior after the election. I wrote about exit-polls earlier; panel conditioning might be one of the reasons why Internet-panels do so badly at predicting election outcomes. Many other studies have focused on panel conditioning: for attitudes, cognitive abilities, knowledge, marital satisfaction and consumer behavior. Use google scholar on &amp;lsquo;practice effect&amp;rsquo;, &amp;lsquo;reactivity&amp;rsquo;, &amp;lsquo;panel conditioning&amp;rsquo;, &amp;lsquo;test-retest effect&amp;rsquo; and you&amp;rsquo;ll see what I mean.&lt;/p&gt;
&lt;p&gt;Overall, the findings suggest that panel conditioning may indeed be problematic, but not in all studies, or for all people. I have some ideas on the circumstances that lead or do not lead to conditioning effects (topic saliency, interval between measurements, frequency of measurement), but none of the studies systematically analyses potential causes for conditioning effects. I am hoping to add some work on this issue in the next years. If anyone know of interesting panel studies that are confronted with panel conditioning effects, let me know&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>how to use Internet panels for polling</title>
      <link>/post/how-to-use-internet-panels-for-polling/</link>
      <pubDate>Thu, 10 Feb 2011 11:13:00 +0100</pubDate>
      <guid>/post/how-to-use-internet-panels-for-polling/</guid>
      <description>&lt;p&gt;Before people believe I&amp;rsquo;m old-fashioned, I do think that Internet-surveys, even panel surveys are the future of survey research. John Krosnick makes some good points in a video shot by the people from 
&lt;a href=&#34;http://www.blogger.com/www.pollster.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.pollster.com&lt;/a&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>predicting elections</title>
      <link>/post/predicting-elections/</link>
      <pubDate>Mon, 17 Jan 2011 17:45:00 +0100</pubDate>
      <guid>/post/predicting-elections/</guid>
      <description>&lt;p&gt;Opinion pollers do a lousy job of predicting elections. For a good read, see for example the prediction of the 
&lt;a href=&#34;http://en.wikipedia.org/wiki/New_Hampshire_Democratic_primary,_2008&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;New Hampshere primary&lt;/a&gt;
 in 2008, when all polls predicted Obama to win, but it was Clinton who won (albeit by a slim margin).&lt;/p&gt;
&lt;p&gt;In the Dutch context, there are three main polling firms, that each do equally well (or badly). Out of a hundred and fifty parliamentary seats, 
&lt;a href=&#34;http://www.blogger.com/www.peil.nl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;peil.nl&lt;/a&gt;
 mispredicted 20, while 
&lt;a href=&#34;http://www.tns-nipo.com/pages/nieuws-pers-politiek-tk2010.asp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TNS-NIPO&lt;/a&gt;
 and 
&lt;a href=&#34;http://www.synovate.nl/content.asp?targetid=621&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synovate&lt;/a&gt;
 shared the honor of only missing the target by 16 seats in the 2010 parliamentary election. These polls were conducted the day before the election, and some of the pollers said that people might have changed their vote at the last minute. That may very well be, but even the exit poll on the night of the election was wrong. 
&lt;a href=&#34;http://en.wikipedia.org/wiki/Dutch_general_election,_2010&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Peil.nl was 17 seats off&lt;/a&gt;
 and 
&lt;a href=&#34;http://en.wikipedia.org/wiki/Dutch_general_election,_2010&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TNS NIPO 15&lt;/a&gt;
. 
&lt;a href=&#34;http://en.wikipedia.org/wiki/Dutch_general_election,_2010&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Only Synovate&lt;/a&gt;
 did a lot better, and only missed the true result by 3 seats. I will discuss why this is in a next post, but it is just a matter of speed and low costs versus quality.&lt;/p&gt;
&lt;p&gt;And we have known for a long, long time how to do exit polls. Although there was public outcry in the UK, when the exit poll 
&lt;a href=&#34;http://news.bbc.co.uk/2/hi/uk_news/politics/election_2010/8666266.stm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;predicted the liberal democrats not to win the elections&lt;/a&gt;
, it was spot on. If we know how to do it, then why don&amp;rsquo;t we?&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
