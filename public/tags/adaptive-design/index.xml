<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>adaptive design | Peter Lugtig</title>
    <link>https://peterlugtig.github.io/tags/adaptive-design/</link>
      <atom:link href="https://peterlugtig.github.io/tags/adaptive-design/index.xml" rel="self" type="application/rss+xml" />
    <description>adaptive design</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019 - 2020</copyright><lastBuildDate>Wed, 20 May 2015 15:00:00 +0200</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>adaptive design</title>
      <link>https://peterlugtig.github.io/tags/adaptive-design/</link>
    </image>
    
    <item>
      <title>Adaptive designs: 4 ways to improve panel surveys</title>
      <link>https://peterlugtig.github.io/post/adaptive-designs-4-ways-to-improve/</link>
      <pubDate>Wed, 20 May 2015 15:00:00 +0200</pubDate>
      <guid>https://peterlugtig.github.io/post/adaptive-designs-4-ways-to-improve/</guid>
      <description>&lt;p&gt;This is a follow-up on why 
&lt;a href=&#34;http://www.peterlugtig.com/2015/01/why-panel-surveys-need-to-go-adaptive.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;I think panel surveys need to adapt their data collection strategies to target individual respondents&lt;/a&gt;
. Let me first note that apart from limiting nonresponse error, there are other reasons why we would want to do this. We can limit survey costs by using expensive survey resources only for people who need them.&lt;br&gt;
A focus on nonresponse alone can be too limited. For example: imagine we want to measure our respondents&amp;rsquo; health. We can maybe do this cheaply by using web interviews, and then try to limit nonresponse error by using interviewers to convert initial nonrespondents. But what about measurement? If we use web surveys we largely have to rely on self-reports on the respondents&amp;rsquo; health. But if we use interviewers for everyone and do a Face-to-Face survey among all our respondents, we can use the interviewers to obtain objective health measures for respondents. These objective measurements could be much better than the self-reports. So face-to-face interviews may not be &amp;lsquo;worth&amp;rsquo; the cost if we look at nonresponse alone, but if we also include the effects on measurement, it may be a viable interview option, if we reduce the sampling size.&lt;br&gt;
 &lt;br&gt;
I think a focus just on any one type of survey error can have adverse effects, and it is Total Survey Error as well as costs we need to keep in mind. Having said this, I really believe nonresponse errors in panel surveys are a huge problem. What could we do (and have others done)?&lt;/p&gt;
&lt;p&gt;1.  Targeted mode. Some respondents are easy to reach in all modes, and some are difficult in all modes. There is also a &amp;lsquo;middle&amp;rsquo; group, who may participate in some modes, but not others. I, for example dislike web surveys (because I get so many), but appreciate mail surveys (because I never get them). In a panel survey, we can ask respondents about mode preferences. Some studies&lt;br&gt;
(
&lt;a href=&#34;http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1200&amp;amp;context=sociologyfacpub&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
, 
&lt;a href=&#34;https://www.melbourneinstitute.com/downloads/conferences/HILDA_2013/HILDA_2013_papers/Kaminska,%20Olena_final%20paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
) have found that stated mode preferences are not very predictive of response in that mode in the next wave, as people indicate to prefer the mode they are interviewed in. This means we probably need a better model than just &amp;lsquo;mode preference&amp;rsquo; to make this work.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://3.bp.blogspot.com/-Ai5RD5cDfno/VVyCKui-NPI/AAAAAAAACvc/i2_khGd_twE/s1600/annoyed%2Bpanel%2Bmember.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://3.bp.blogspot.com/-Ai5RD5cDfno/VVyCKui-NPI/AAAAAAAACvc/i2_khGd_twE/s320/annoyed%2Bpanel%2Bmember.jpg&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Probably wants a different survey mode next time.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
2.  Targeted incentives. We know some respondents are &amp;lsquo;in it&amp;rsquo; for the money, or at least are sensitive to offers of incentives. In panel surveys, we can learn quickly about this by experimenting with amounts both between and/or within persons. For example, does it help to offer higher incentives to hard-to-reach respondents? Does that help just once, or is there a persistent effect? It may be unethical to offer higher incentives to just hard-to-reach respondents, as we then put a premium on bad respondent behavior. We could however use different metrics for deciding whom to offer the incentive. Nonresponse bias is a much better indicator for example. There is not too much we know about how to do this, although there is a nice example 
&lt;a href=&#34;http://poq.oxfordjournals.org/content/77/3/696&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here.&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;3. Targeted advance letters. We know quite a lot  about the effects different types of advance letters have on subsequent response. Young people can for example be targeted with a letter with a flashy lay-out and short bites of information of the study, while older people may prefer a more &amp;lsquo;classic&amp;rsquo; letter with more extensive information about the study setup and results.&lt;br&gt;
The effects of targeted letters on response in panel surveys are often small, and only present for specific subgroups. See 
&lt;a href=&#34;https://www.understandingsociety.ac.uk/research/publications/working-paper/understanding-society/2014-08.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
, and 
&lt;a href=&#34;http://www.risq-project.eu/papers/luiten-schouten-2013.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
 for two examples. Still, this type of targeting costs little, perhaps we can find bigger effects if we know what groups to target with what message. As with other targeting methods, we need a combination of data mining and experimentation to develop knowledge about this.&lt;/p&gt;
&lt;p&gt;4. Targeted tracking. I am not aware of any survey doing targeted tracking. Tracking is done during fieldwork. Respondents who are not located by an interviewer (or advance letter which bounce), are sent back to the study coordinating team, after which tracking methods are used to locate the respondent at an alternative address. From the literature we know that it is mainly people who move house who need tracking. If we can successfully predict the likelihood to move, we could potentially save time (and money) in fieldwork, by putting cases into preventive tracking. We could also potentially use a targeted order of tracking procedures, 
&lt;a href=&#34;http://jameswagnersurv.blogspot.nl/2014/05/tracking-does-sequence-matter.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;as James Wagner has done.&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://poq.oxfordjournals.org/content/77/3/696&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;/a&gt;
&lt;br&gt;
-&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>why panel surveys need to go &#39;adaptive&#39;</title>
      <link>https://peterlugtig.github.io/post/why-panel-surveys-need-to-go-adaptive/</link>
      <pubDate>Wed, 21 Jan 2015 15:21:00 +0100</pubDate>
      <guid>https://peterlugtig.github.io/post/why-panel-surveys-need-to-go-adaptive/</guid>
      <description>&lt;p&gt;Last week, I gave a talk at Statistics Netherlands (slides 
&lt;a href=&#34;https://www.dropbox.com/s/ul9msor9d6f9ak7/Lugtig%20-%20panel%20dropout%20%28talk%20at%20CBS%20January%202015%29.ppt.pdf?dl=0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
) about panel attrition. Initial and nonresponse and dropout from panel surveys have always been a problem. A famous study by Groves and Peytcheva (
&lt;a href=&#34;http://poq.oxfordjournals.org/content/70/5/646&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
) showed that in cross-sectional studies, nonresponse rates and nonresponse bias are only weakly correlated. In panel surveys however, all the signs are there that dropout in a panel study is often related to change. Those respondents undergoing most change, are also most likely to drop out. This is probably partly because of respondents (e.g. a move of house could be a good reason to change other things as well, like survey participation), but it is also because of how surveys deal with such moves. Movers are much harder to contact (if we don&amp;rsquo;t have accurate contact details anymore). Movers are often assigned to a different interviewer. This will all lead to an underestimate of the number of people who move house in panel studies. Moving house is associated with lots of other life events (change in household composition, change in work, income etc.). In short dropout is a serious problem in longitudinal studies.&lt;/p&gt;
&lt;p&gt;The figure below shows the cumulative response rates for some large-scale panel studies. The selection of panel studies is a bit selective. I have tried to focus on large panel studies (so excluding cohort studies), which are still existing today, with a focus on Western Europe. &lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://3.bp.blogspot.com/-hQrr9XFijnI/VL-0qshxygI/AAAAAAAACts/lG8A-ZdO2Ho/s1600/Rplot08.tiff&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://3.bp.blogspot.com/-hQrr9XFijnI/VL-0qshxygI/AAAAAAAACts/lG8A-ZdO2Ho/s1600/Rplot08.tiff&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Cumulative nonresponse rates in large panel surveys (click to enlarge)&lt;/p&gt;
&lt;p&gt;The oldest study in the figure (PSID) has the highest initial response rate, followed by studies which were started in the 1980s (GSOEP), 1990s (BHPS), and early 2000s (HILDA). The more recent studies all have higher initial nonresponse rates. But not only that. They also have higher dropout rates (the lines go down much faster). This is problematic.&lt;/p&gt;
&lt;p&gt;I think these differences are not due to the fact that we, as survey methodologists, are doing a worse job now as compared to 20 years ago. If anything, we have been trying use more resources, professionalize tracking, offer higher incentives, and be more persistent. In my view, the increasing dropout rates are due to changes in society (the survey climate). A further increase of our efforts (e.g. higher incentives) could perhaps help somewhat to reduce future dropout. I think this is however not the way to go, especially as budgets for data collection face pressures everywhere.&lt;/p&gt;
&lt;p&gt;The way to reduce panel dropout is to collect data in a smarter way. First, we need to understand why people drop out. This is something we know quite well (but more can be done). For example, we know that likely movers are at risk. So, what we need are tailored strategies that focus on specific groups of people (e.g. likely movers). For example, we could send extra mailings in between waves only to them. We could use preventive tracking methods. We could put these into the field earlier.&lt;/p&gt;
&lt;p&gt;I am not the first to suggest such strategies. We have been tailoring our surveys for ages to specific groups, but have mostly done so at an ad-hoc basis,  never systematically. 
&lt;a href=&#34;http://jameswagnersurv.blogspot.nl/2010/09/responsive-design-and-adaptive-design.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Responsive or adaptive designs try to use tailoring systematically&lt;/a&gt;
, for those groups that most benefit from tailoring. Because we know so much about our respondents after wave 1, panel studies offer lots of opportunities to implement responsive designs.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
