<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data quality | Peter Lugtig</title>
    <link>https://thomvolker.github.io/tags/data-quality/</link>
      <atom:link href="https://thomvolker.github.io/tags/data-quality/index.xml" rel="self" type="application/rss+xml" />
    <description>data quality</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019 - 2020</copyright><lastBuildDate>Tue, 26 Jun 2018 11:16:00 +0200</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>data quality</title>
      <link>https://thomvolker.github.io/tags/data-quality/</link>
    </image>
    
    <item>
      <title>Which survey error source is larger: measurement or nonresponse?</title>
      <link>https://thomvolker.github.io/post/which-survey-error-source-is-larger/</link>
      <pubDate>Tue, 26 Jun 2018 11:16:00 +0200</pubDate>
      <guid>https://thomvolker.github.io/post/which-survey-error-source-is-larger/</guid>
      <description>&lt;p&gt;As a survey methodologist I get paid to develop survey methods that generaly minimize survey errors, and advise people on how to field surveys in a specific setting. A question that has been bugging me for a long time is what survey error we should worry about most. The 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Total_survey_error&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Total Survey Error (TSE) framework&lt;/a&gt;
 is very helpful for thinking which type of survey error may impact survey estimates&lt;br&gt;
But which error source is generally larger?  Nonresponse or measurement errors?&lt;/p&gt;
&lt;p&gt;Thankfully, no one has ever asked me this question yet, because I would find it impossible to answer anything then &amp;ldquo;well, that depends&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The reason why we don&#39;t know what error source is larger is that we can usually assess observational errors only for the people we have actually observed. There are several ways to do this. Sometimes we know the truth, and so we can compare survey answers (&amp;ldquo;do you have a valid driver&#39;s license?&amp;quot;) to data that we know from administrative records. If we are interested in attitudes, we can use psychometric models. The people behind the 
&lt;a href=&#34;http://sqp.upf.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;computer programme SQP&lt;/a&gt;
 have summarised a huge number of question experiments and 
&lt;a href=&#34;http://davidakenny.net/cm/mtmm.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MTMM models&lt;/a&gt;
 to predict the quality of a specific survey questions. By asking different forms of the same question (e.g. how interested are you in politics?&amp;quot;) we can gauge the reliability and validity of this question under different question wordings and answer scales.&lt;/p&gt;
&lt;p&gt;The problem of course is that if we are indeed interested in the concept &amp;ldquo;interest in politics&amp;rdquo;, we would ideally also like to know what people who we have not observed would have answered. In order to estimate errors of non-observation (nonresponse), we would need to  actually observe these people!&lt;/p&gt;
&lt;p&gt;There are of course some situations where we actually do know something about nonrespondents. 
&lt;a href=&#34;https://www.jstor.org/stable/2746919?seq=1#page_scan_tab_contents&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cannell and Fowler (1963) are an early example:&lt;/a&gt;
 they knew something about nonrespondents (hospital visits) and could compare different respondent and nonrespondent groups. A more recent great example is by 
&lt;a href=&#34;https://academic.oup.com/poq/article-abstract/74/5/880/1816288?redirectedFrom=PDF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kreuter, Muller and Trappmann (2010)&lt;/a&gt;
. They did a survey among people for whom they already knew their employment status. They showed that nonresponse and measurement error in employment status were about of equal size, and go in different directions.&lt;/p&gt;
&lt;p&gt;There are several other studies, among 
&lt;a href=&#34;https://academic.oup.com/poq/article/74/5/907/1815368&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;students&lt;/a&gt;
 or in the context of 
&lt;a href=&#34;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=3&amp;amp;ved=0ahUKEwjo8LmT_PDbAhUHsaQKHdBpCIsQFghAMAI&amp;amp;url=https%3A%2F%2Fwww.cbs.nl%2F-%2Fmedia%2Fimported%2Fdocuments%2F2015%2F45%2F2015-evaluating-bias-of-sequential-mixed-mode-designs-against-benchmark-surveys.pdf&amp;amp;usg=AOvVaw2DE1n5X7Rs0kmQyzcLJ6vg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mixed-mode studies&lt;/a&gt;
 that have looked at factual questions, and estimated both measurement and nonresponse error in the same study. So, what do we learn? From my reading of the literature, there is no clear pattern in findings. Sometimes measurement errors are larger, sometimes nonresponse is larger. And sometimes these survey errors go in the same direction, and sometimes in different directions. A further problem is that these validation studies use factual questions, not attitudinal questions, which surveys are more often interested in. In conclusion, that means that:&lt;/p&gt;
&lt;p&gt;1. For factual questions, is is not clear whether nonresponse or measurement errors are the larger problem. There is large variation across studies.&lt;br&gt;
2. Because the measurement quality of attitudinal questions is generally lower than that of factual questions, measurement errors may pose a relatively larger problem than nonresponse in attitudinal questions.&lt;br&gt;
3. BUT, we then have to assume that nonresponse bias is generally the same for attitudinal and factual questions, which may not be true. 
&lt;a href=&#34;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwi674n2_fDbAhVFZlAKHQQ1ChAQFggzMAE&amp;amp;url=https%3A%2F%2Fwww.scp.nl%2Fdsresource%3Fobjectid%3Da9cda030-1389-4d05-877b-c2f429d821f5%26type%3DPDF&amp;amp;usg=AOvVaw2dDdY8srg0XUTG4Cj59eRd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stoop (2005)&lt;/a&gt;
 and 
&lt;a href=&#34;https://www.jstor.org/stable/25791719?seq=1#page_scan_tab_contents&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;others&lt;/a&gt;
 have shown that if you are interested in measuring in &amp;quot; interest in politics&amp;rdquo;, late and hard-to-reach respondents are very different from early and easy respondents.&lt;/p&gt;
&lt;p&gt;So, what to do? How do we make progress so that I can at some point give an answer to the question which error sourcewe should worry about most?&lt;/p&gt;
&lt;p&gt;1. We could find studies with a very high response rate (100% ideally) and study the differences between the easy and late respondents, like Stoop did.&lt;br&gt;
2. We should do more validation studies for factual questions, which should become more feasible, as more and more register data are available.&lt;br&gt;
3. And, we should try to link MTMM studies and other psychometric models to nonresponse models. I recently 
&lt;a href=&#34;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwiO4b2B__DbAhWRJVAKHYXiBW8QFggoMAA&amp;amp;url=https%3A%2F%2Fojs.ub.uni-konstanz.de%2Fsrm%2Farticle%2FviewFile%2F7170%2F6532&amp;amp;usg=AOvVaw0cbWfuZKrT7PcCL7Tm-1yi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;did a study that did this for a panel study,&lt;/a&gt;
 but what is really needed is work in cross-sectional studies.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Retrospective reporting</title>
      <link>https://thomvolker.github.io/post/retrospective-reporting/</link>
      <pubDate>Tue, 01 Dec 2015 13:51:00 +0100</pubDate>
      <guid>https://thomvolker.github.io/post/retrospective-reporting/</guid>
      <description>&lt;p&gt;Back after a long pause. Panel surveys traditionally interview respondents at regular intervals, for example monthly or yearly. This interval is mostly chosen for practical reasons: interviewing people more frequently would lead to a large respondent burden, and a burden on data processing and dissemination. For these practical reasons, panel surveys often space their interviews one year apart. Many of the changes (e.g. changes in household composition) we as researchers are interested in occur slowly, and annual interviews suffice to capture these changes.&lt;br&gt;
Sometimes we want to get reports at a more detailed level, however. For example, we would like to know how often a respondents visits a doctor (general practitioner) in one year, or when a respondent went on holidays. In order to get at such an estimate, survey researchers can do one of several things:&lt;/p&gt;
&lt;p&gt;1. we can ask a respondent to remember all doctor visits in the past year, and rely on &lt;strong&gt;retrospective recall&lt;/strong&gt;. We know that doesn&#39;t work very well, because respondents cannot remember all visits. Instead, respondents will rely on rounding, guessing and estimation to come to an estimate.&lt;br&gt;
2. we can ask respondents for visits in say the past month, and rely on &lt;strong&gt;extrapolation&lt;/strong&gt; to get to an annual estimate. This strategy works well if doctor visits are stable throughout the year (which they are not).&lt;br&gt;
3. We can try to break down the year into months, and instead of asking for doctor visits in the last year, ask for doctor visits in each month, reducing the &lt;strong&gt;reference period&lt;/strong&gt;. We can stimulate the retrieval of the correct information further by using a timeline or by the use of landmarks.&lt;br&gt;
4. We can interview respondents more often. So, we conduct 12 monthly interviews instead of one annual interview, thereby reducing both the reference and &lt;strong&gt;recall period.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With 
&lt;a href=&#34;https://www.linkedin.com/in/tinaglasner&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tina Glasner&lt;/a&gt;
 and 
&lt;a href=&#34;https://research.vu.nl/en/persons/anja-boev%C3%A9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anja Boeve&lt;/a&gt;
, I recently published a paper that compared methods 1, 3 and 4 within the same study to estimate the total number of doctor (family physician) visits. This study is unique in the sense that we used all three methods within the same respondents, so for each respondents we can see how reporting is different when we rely on annual recall, monthly recall, and on whether we use an annual or monthly reference period. Our study also included an experiment to see whether timelines and landmarks improved recall.&lt;/p&gt;
&lt;p&gt;You can find the full paper 
&lt;a href=&#34;http://ijpor.oxfordjournals.org/content/early/2015/09/05/ijpor.edv032&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;We find that respondents give different answers about their doctor visits depending on how we ask them. The estimates for annual visits are;&lt;br&gt;
- annual estimate (1 question): 1.96 visits&lt;br&gt;
- monthly estimate with 12-month recall: 2.62 visits&lt;br&gt;
- monthly estimate with 1-month recall: 3.90 visits&lt;/p&gt;
&lt;p&gt;The average number of doctor visits in  population registers is 4.66, so the monthly estimate with 1 month-recall periods comes closest to our population estimate.&lt;/p&gt;
&lt;p&gt;As a final step, we were interested in understanding which respondents give different answers depending on the question format. For this, we studied the within-person absolute difference between the monthly estimates with a 12-month and 1-month recall period. The table below shows that the more frequent doctor visits are, the larger the differences between the 1 month and 12-month recall periods. This implies that respondents with more visits tend to underreport them more often when the recall period is long. The same holds for people in moderate and good health. People in bad health often go to the doctor regularly, and remember these visits. More infrequent visits are more easily forgotten. Finally, we find that the results of our experiment are non-significant. Offering respondents personal landmarks, and putting these next to the timeline to improve recall, does not lead to smaller differences.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://4.bp.blogspot.com/-aVfXfHTFn90/Vl19JIYKxEI/AAAAAAAACxY/Ec917NXE9eU/s1600/table%2Bfrom%2B%2Bijpor%2B%25282015%2529.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://4.bp.blogspot.com/-aVfXfHTFn90/Vl19JIYKxEI/AAAAAAAACxY/Ec917NXE9eU/s640/table%2Bfrom%2B%2Bijpor%2B%25282015%2529.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;In practice, these findings may be useful when one is interested in estimating frequencies of behavior over an annual period. Breaking up the &amp;lsquo;annual-estimate&amp;rsquo; question into twelve monthly questions helps to improve data quality. Asking about such frequencies at 12 separate occasions further helps, but this is unlikely to be feasible due to the large increased costs of more frequent data collection. In self-administered web-surveys this might however be feasible. Splitting up questionnaires into multiple shorter ones may not only reduce burden, but can increase data quality for specific survey questions as well.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Satisficing in mobile web surveys. Device-effect or selection effect?</title>
      <link>https://thomvolker.github.io/post/blog-post/</link>
      <pubDate>Mon, 08 Dec 2014 20:47:00 +0100</pubDate>
      <guid>https://thomvolker.github.io/post/blog-post/</guid>
      <description>&lt;p&gt;Last week, I wrote about the fact 
&lt;a href=&#34;http://www.peterlugtig.com/2014/12/what-devices-do-respondents-use-over.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;that respondents in panel surveys are now using tablets and smartphones to complete web surveys&lt;/a&gt;
. We found that in the LISS panel, respondents who use tablets and smartphones are much more likely to switch devices over time and not participate in some months.&lt;br&gt;
The question we actually wanted to answer was a different one: do respondents who complete surveys on their smartphone or mobile give worse answers?&lt;/p&gt;
&lt;p&gt;To do this, we used 6 months of data from the LISS panel, and in each month, coded the User Agent String. We then coded types of satisficing behavior that occur in surveys: the percentage of item missings, whether respondents complete (non-mandatory) open questions, how long their answers were, whether respondents straightline, whether they go for the first answers in a check-all-that-apply questions, and how many answers they click in a check-all-that apply question. We also looked at interview duration, and how much respondents liked the survey.&lt;/p&gt;
&lt;p&gt;We found that respondents on a smartphone seem to do much worse. They take longer to complete the survey, are more negative about the survey, have more item missings, and have a much higher tendency to pick the first answer. On the other questions, differences were small, sometimes in favor of the smartphone user.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://3.bp.blogspot.com/-ugtwA4jujIY/VIYBnhikpVI/AAAAAAAACs4/y_99X9lD1Aw/s1600/Slide1.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://3.bp.blogspot.com/-ugtwA4jujIY/VIYBnhikpVI/AAAAAAAACs4/y_99X9lD1Aw/s1600/Slide1.jpg&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Click to enlarge: indicators of satisficing per device in LISS survey&lt;/p&gt;
&lt;p&gt;Is this effect due to the fact that the smartphone and tablet are not made to complete surveys, and is satisficing higher because of a device-effect? Or is it a person effect, and are worse respondents more inclined to do a survey on a tablet or smartphone?&lt;/p&gt;
&lt;p&gt;In order to answer this final question, we looked at device-transitions that respondents take within the LISS panel. In the 6 months of the LISS, respondents can make 5 transitions from using 1 device in the one month, to another (or the same) device in the next. For 7 out of 9 transitions (we have too few observations to analyze the tablet -&amp;gt; phone and phone -&amp;gt; tablet transitions), we can then look at the difference in measurement error that is associated with a change in device.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://3.bp.blogspot.com/-xyw5vo1H-28/VIYFOBkuBpI/AAAAAAAACtM/knp91jodOE4/s1600/plotbars3.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://3.bp.blogspot.com/-xyw5vo1H-28/VIYFOBkuBpI/AAAAAAAACtM/knp91jodOE4/s1600/plotbars3.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Click to enlarge. Changes in data quality (positive is better) associated with change in device.&lt;/p&gt;
&lt;p&gt;The red bars indicate that there is no significant change in measurement error associated with a device change. Our conclusion is that device changes do not lead to more measurement error, with 2 exceptions:&lt;br&gt;
1. A transition from tablet -&amp;gt; PC or phone -&amp;gt; PC in two consecutive months, leads to a better evaluation of the questionnaire. This implies that the user experience of completing web surveys on a mobile device should be improved.&lt;br&gt;
2. We find that people check more answers in a check-all-that-apply question when they move from a tablet -&amp;gt; PC, or phone -&amp;gt; PC&lt;/p&gt;
&lt;p&gt;So, in short. Satisficing seems to be more problematic when surveys are completed on tablets and phones. But this can almost fully be explained by a selection effect. Those respondents who are worse completing surveys, choose to complete surveys more on tablets and smartphones.&lt;/p&gt;
&lt;p&gt;The full paper can be found 
&lt;a href=&#34;https://www.dropbox.com/s/ew6rtantczkpi7y/Lugtig%20and%20Toepoel%20%28prepublication%29.pdf?dl=0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>To weight or to impute for unit nonresponse?</title>
      <link>https://thomvolker.github.io/post/to-weight-or-to-impute-for-unit/</link>
      <pubDate>Sun, 06 Oct 2013 21:49:00 +0200</pubDate>
      <guid>https://thomvolker.github.io/post/to-weight-or-to-impute-for-unit/</guid>
      <description>&lt;p&gt;This week, I have been reading the most recent issue of the 
&lt;a href=&#34;http://www.jos.nu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Journal of Official Statistics&lt;/a&gt;
, a journal that has been open access since the 1980s.  In this issue is a 
&lt;a href=&#34;http://www.jos.nu/Articles/abstract.asp?article=293329&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;critical review article of weighting procedures&lt;/a&gt;
 authored by Michael Brick with commentaries by Olena Kaminska (
&lt;a href=&#34;http://www.jos.nu/Articles/abstract.asp?article=293355&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
), Philipp Kott (
&lt;a href=&#34;http://www.jos.nu/Articles/abstract.asp?article=293359&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
), Roderick Little (
&lt;a href=&#34;http://www.jos.nu/Articles/abstract.asp?article=293363&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
), Geert Loosveldt (
&lt;a href=&#34;http://www.jos.nu/Articles/abstract.asp?article=293367&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
), and a rejoinder (
&lt;a href=&#34;http://www.jos.nu/Articles/abstract.asp?article=293371&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
).&lt;/p&gt;
&lt;p&gt;I found this article a great read, and to be full of ideas related to unit nonresponse. The article reviews approaches to weighting: either to the sample or the population, by poststratification and with different statistical techniques. But it discusses much more, and I recommend reading it.&lt;/p&gt;
&lt;p&gt;One of the issues that is discussed in the article, but much more extensively in a commentary by Roderick Little, is the question whether we should use weighting or imputations to adjust for unit nonresponse in surveys. Over the years, I have switched allegiances to favouring weighting or imputations in certain missing data situations many times, and I am still not always certain on what is best to do. Weighting is generally favoured for cross-sectional surveys, because we understand how it works. Imputations are generally favoured when we have strong correlates for missingness and our variable(s) of interest, such as in longitudinal surveys. Here are some plusses and minuses for both weighting and imputations.&lt;/p&gt;
&lt;p&gt;Weighting is design based. Based on information that is available for the population or whole sample (including nonrespondents), respondent data are weighted in such a way that the survey data reflect the sample/population again.&lt;/p&gt;
&lt;p&gt;+ The statistical properties of all design-based weighting procedures are well-known.&lt;br&gt;
+ Weighting works with complex sampling designs (at least theoretically).&lt;br&gt;
+ We need relatively little information on nonrespondents to be able to use weighting procedures. There is however a big BUT&amp;hellip;&lt;br&gt;
- Weighting models mainly use socio-demographic data, because that is the kind of information we can add to our sampling frame. These variables are never highly correlated with our variable of interest, nor missingness due to nonresponse, so weighting is not very effective. That is, weighting theoretically works nicely, but in practice, it doesn&#39;t ameliorate the missing data problem we have because of unit nonresponse much.&lt;/p&gt;
&lt;p&gt;Imputations are model based. Based on available information for respondents and nonrespondents, a prediction model is built for a variable which has missing information. The model can take an infinite number of shapes, depending on whether imputation is stochastic, how variables are related within the model, and what variables are being used. Based on this model, one or multiple values are imputed for every missing value on every variable for every case. The crucial difference is that weighting uses the same variables for correcting the entire dataset, whereas imputation models differ for every variable that is to be imputed.&lt;/p&gt;
&lt;p&gt;+ Imputation models are flexible. This means that the imputation model can be optimized in such a way that it strongly predicts both the dependent variable to be imputed, and the missingness process.&lt;/p&gt;
&lt;p&gt;- In the case of unit nonresponse, we often have limited data on nonrespondents. So, although a model-based approach may have advantages over design-based aproaches in terms of its ability to predict our variable(s) of interest, this depends on the quality of the covariates we use.&lt;/p&gt;
&lt;p&gt;This then brings me, and the authors of the various papers in JoS back to the basic problem: 
&lt;a href=&#34;http://www.peterlugtig.com/2013/09/nonresponse-workshop-2013.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;we don&#39;t understand the process on nonresponse in surveys&lt;/a&gt;
. Next time, more on imputations and weighting for longitudinal surveys. And more on design vs. model based approaches in survey research.&lt;/p&gt;
&lt;p&gt;p.s. This all assumes simple random sampling. If complex sampling designs are used, weighting is until now I think the best way to start dealing with nonresponse. I am unaware of imputation methods that can deal with complex sampling (other than straightforward multilevel structures).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Publish your data</title>
      <link>https://thomvolker.github.io/post/publish-your-data/</link>
      <pubDate>Mon, 23 Sep 2013 12:27:00 +0200</pubDate>
      <guid>https://thomvolker.github.io/post/publish-your-data/</guid>
      <description>&lt;p&gt;This morning, an official enquiry into the scientific conduct of 
&lt;a href=&#34;http://en.wikipedia.org/wiki/Mart_Bax&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;professor Mart Bax&lt;/a&gt;
 concluded that he had committed large-scale scientific fraud over a period of 15 years. Mart Bax is a now-retired professor of political anthropology at the Free University Amsterdam. In 2012 a journalist first accused him of fraud, and this spring, the Volkskrant, one of the big newspapers in the Netherlands reported they were not able to find any of the informants Mart Bax had used in his studies.&lt;/p&gt;
&lt;p&gt;An official enquiry followed. You can can the report 
&lt;a href=&#34;https://www.dropbox.com/s/pjrwxvbw175cpo1/rapport%20Bax.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here (in Dutch)&lt;/a&gt;
. In summary, Mart Bax most likely made up at least 64, mostly peer-reviewed articles and recycled his own articles using different titles in different journals. Although the investigation could not rule out that some studies were just done sloppily the overall picture from the report is one of overall scientific misconduct.&lt;/p&gt;
&lt;p&gt;So, what to do about this? I have a clear opinion on this: 
&lt;a href=&#34;http://www.peterlugtig.com/2013/09/how-to-improve-social-sciences.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Make your data available, and replicate other people&#39;s studies&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;What strikes me, is that it seems normal to some social scientists not to store interviews (whether on tape or anonymized in scripts) or publish datasets. It may be a little more difficult for qualitative researchers than quantitative researchers to do this. Back in the 1990s, when Mart Bax committed this fraud, it may have been really complicated to publish such transcripts online. Nowadays, it is dead easy however, and some, although not many journals offer this service. See for some good examples in the social sciences:&lt;/p&gt;
&lt;p&gt;- 
&lt;a href=&#34;http://www.oxfordjournals.org/our_journals/restud/for_authors/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The review of economical studies&lt;/a&gt;
: will only publish articles that provide data, and where analyses can be replicated. This is the only example that I could find of a journal policy that really makes it easy to replicate research findings.&lt;br&gt;
- 
&lt;a href=&#34;http://www.springer.com/authors/manuscript&amp;#43;guidelines?SGWID=0-40162-12-339499-0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;All Springer journals&lt;/a&gt;
 provide the opportunity for supplementary materials (among them data). Is just an option.&lt;br&gt;
- 
&lt;a href=&#34;http://www.apa.org/pubs/journals/psp/index.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The journal of Personality and Social Psychology&lt;/a&gt;
 encourages providing data and analysis scripts in general (The American Psychological Association as w hole does this).&lt;/p&gt;
&lt;p&gt; If you know of any other journals with good replicability policies, please send a comment or e-mail (p.lugtig AT uu.nl), so I can compile a more comprehensive list.&lt;br&gt;
 &lt;br&gt;
In the natural sciences, there are 
&lt;a href=&#34;http://proj.badc.rl.ac.uk/preparde/blog/DataJournalsList&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;journals where datasets are peer-reviewed and publishe&lt;/a&gt;
d. As social scientists, we have a long way to go to tackle fraud, and generally become much more open about our data and analysis methods. Journals, professional associations, and individualresearchers should all be stricter on data accessibility, and replicability of studies.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to improve the social sciences</title>
      <link>https://thomvolker.github.io/post/how-to-improve-social-sciences/</link>
      <pubDate>Sat, 14 Sep 2013 15:37:00 +0200</pubDate>
      <guid>https://thomvolker.github.io/post/how-to-improve-social-sciences/</guid>
      <description>&lt;p&gt;Social scientists (and psychology in particular) have in recent years had somethings of a bad press, both in- and outside academia. To give some examples:&lt;/p&gt;
&lt;p&gt;- There is a sense among some people that 
&lt;a href=&#34;http://www.socialsciencespace.com/2013/07/attacks-on-us-federal-funding-of-the-social-sciences-will-continue-to-intensify/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;social science provides little societal or economical value.&lt;/a&gt;
&lt;br&gt;
- Controversy over research findings within social science: for example the findings of 
&lt;a href=&#34;http://www.thepsychologist.org.uk/archive/archive_home.cfm?volumeID=25&amp;amp;editionID=213&amp;amp;ArticleID=2059&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bem et al. about the existence of precognition&lt;/a&gt;
, or the 
&lt;a href=&#34;http://en.wikipedia.org/wiki/Casualties_of_the_Iraq_War&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;estimation of the number of casualties in Iraq war&lt;/a&gt;
 (2003-2007).&lt;br&gt;
- Scientific fraud: in the Netherlands alone, we have had about five affairs of scientific fraud in the past few years. The biggest one being the 
&lt;a href=&#34;http://en.wikipedia.org/wiki/Diederik_Stapel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stapel-affair:&lt;/a&gt;
 a professor who dreamed up the data of about 50 high-profile scientific articles.&lt;/p&gt;
&lt;p&gt;Now, I disagree with the fact that the social sciences do not contribute enough to society, but is hard to argue about this issue if people can argue that social scientists commit fraud, or are secretive about their methods and results. Also, I dislike the 
&lt;a href=&#34;http://www.easp.eu/news/Statement%20EASP%20on%20Levelt_December_%202012.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;defensive attitude many social scientists&lt;/a&gt;
 take on fraud, opnness, and replicability. Instead of just being defensive, try do some something constructive with critique we receive as a field.&lt;/p&gt;
&lt;p&gt;At this 
&lt;a href=&#34;http://www.gesis.org/en/events/gesis-summer-school/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;year&#39;s GESIS summer school,&lt;/a&gt;
 I gave a talk about the topic of survey errors and how to improve social science as a whole. I think it is really time to change the way we as social scientists do our work. We have to be more open about what we do and how we do it. Not only so we can strenghthen the position of the social sciences in general, but more importantly to make progress as a field. Many theories in the social sciences are founded on empirical research findings, that can either not be replicated, are based on wrong statistical analyses, or are based on fraudulated data. Dreaming up data is the worst example of fraud, but I count as fraud also selectively deleting those cases that don&#39;t support your theory, or presenting exploratory analyses as confirmatory. This last point especially is common throughout the social sciences (and in other disciplines as well).&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://3.bp.blogspot.com/-8jF-Uu_-d5A/UjRlcO3LhUI/AAAAAAAACm0/R1nCSlaMRnA/s1600/graphic2-for-blog3.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://3.bp.blogspot.com/-8jF-Uu_-d5A/UjRlcO3LhUI/AAAAAAAACm0/R1nCSlaMRnA/s400/graphic2-for-blog3.jpg&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;_Summaries of a survey on scientific fraud in medical faculties in the Flanders region of Belgium _&lt;br&gt;
_(source: &lt;a href=&#34;http://blogs.scientificamerican.com/talking-back/2013/05/02/spring-and-scientific-fraud-is-busting-out-all-over/)_&#34;&gt;http://blogs.scientificamerican.com/talking-back/2013/05/02/spring-and-scientific-fraud-is-busting-out-all-over/)_&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here is my idea of how to start making things better:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Publish your data&lt;/strong&gt;. Unless you work with medical or otherwise privacy-sensitive data, I see no reason why not all data should be published online at some point. Of course, data need to be cleaned after data collection, and researchers may keep data for themselves for a limited amount of time, if they fear others may steal their good research ideas. But once a paper is published, the data should become available. No excuses&lt;br&gt;
&lt;strong&gt;2. Document your analyses.&lt;/strong&gt; If you publish your data, why not publish your analyses scripts and logs of your analyses as well, so everyone can easily replicate your analyses?  We need more replication in the social sciences, and should make it more attractive for researchers to do this. Yes, that means that data analysis errors may be exposed by other researchers. But in my view, this is the only way we can progress as social sciences.&lt;/p&gt;
&lt;p&gt;And as I believe in practice what you preach, I am putting all my data and analysis-scripts at this webpage (see 
&lt;a href=&#34;http://www.peterlugtig.com/p/publications.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;publications&lt;/a&gt;
). As I do not own all the data, I am still working out how to do this with some of my co-authors, but my goal is to really have all my data and scripts/syntaxes there, so everyone can replicate my analyses given they use the same software.&lt;/p&gt;
&lt;p&gt;The slides for my presentation at GESIS are found 
&lt;a href=&#34;https://www.dropbox.com/s/x3oowfh3rggfw7m/Lugtig%20-%20why%20care%20about%20survey%20errors%20%28GESIS%29%20print%20version.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
. There is also a video-recording of the lecture, that can be found 
&lt;a href=&#34;https://www.youtube.com/watch?v=oGF0ViFphKQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dependent Interviewing and the risk of correlated measurement errors</title>
      <link>https://thomvolker.github.io/post/dependent-interviewing-and-risk-of/</link>
      <pubDate>Sat, 03 Aug 2013 16:22:00 +0200</pubDate>
      <guid>https://thomvolker.github.io/post/dependent-interviewing-and-risk-of/</guid>
      <description>&lt;p&gt;Longitudinal surveys ask the same people the same questions over time. So questionnaires tend to be rather boring for respondents after a while. &amp;ldquo;Are you asking me this again, you asked that last year as well!&amp;rdquo; is what many respondents probably think during an interview. As methodologists who manage panel surveys, we know this process may be rather boring, but in order to document change over time, we just need to ask respondents the same questions over and over.&lt;/p&gt;
&lt;p&gt;Some measures of change over time would become biased if we just repeat questions year-on-year. For example, we know that if we ask respondents twice about their occupation, less than half of all of them have the same occupational codes over time. We know from other statistics (e.g. tax returns), that that is not true. Most people stay in the same occupation over time. Now, you may think, dear reader, that that is probably due to the fact that occupation is rather difficult to measure and code in general, and you are right. Unreliable question will lead to a lot of spurious change over time.&lt;/p&gt;
&lt;p&gt;Dependent Interviewing helps to make codes consistent over time and reduce such spurious change. The idea is that, instead of coding occupation independently year-on-year, you ask respondents in year 2 the question &amp;ldquo;last year, you said you were a bankteller, is that still the case?&amp;quot;. There are many different variants to ask this Dependent Interviewing question, and the exact wording is important for the outcomes. Especially, because we do not want respondent to say &amp;ldquo;yes&amp;rdquo; too easily to questions we ask.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://1.bp.blogspot.com/-Nn9LXqCRaXc/Uf0PNO7KjdI/AAAAAAAACmM/-cLYJqtwcjw/s1600/interview.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://1.bp.blogspot.com/-Nn9LXqCRaXc/Uf0PNO7KjdI/AAAAAAAACmM/-cLYJqtwcjw/s1600/interview.jpg&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&amp;ldquo;Last year, you told me you told me you worked as a bankteller, is that still the case?&amp;quot;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Recently, a paper I wrote on the effects of various forms of Dependent Interviewing came out in 
&lt;a href=&#34;https://dl.dropboxusercontent.com/u/2839696/Lugtig%20and%20Lensvelt-Mulders%20%28preprint%29%20evaluating%20the%20effect%20of%20DI%20on%20the%20quality%20of%20measures%20of%20change.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Field Methods&lt;/a&gt;
. It was actually the first paper I wrote for my Ph.D, and I started work on it in 2006. So, it has been quite a journey to get this story on paper and get it published. I am very happy to see it on paper now. We did an experiment, where we tried out different DI-designs in a four-wave panel study, to study effects of data quality of each different DI-design. Specifically, we looked at whether respondents might falsely confirm data from the previous year that we knew contained measurement error. The bottom line of the study is that when Dependent Interviewing is applied to income amount questions over time, it does improve data quality, and we don&#39;t need to worry so much about respondents wrongly agreeing to pre-loaded data from the  previous year. Read the full paper 
&lt;a href=&#34;https://dl.dropboxusercontent.com/u/2839696/Lugtig%20and%20Lensvelt-Mulders%20%28preprint%29%20evaluating%20the%20effect%20of%20DI%20on%20the%20quality%20of%20measures%20of%20change.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here.&lt;/a&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interested in new mixed mode research project?</title>
      <link>https://thomvolker.github.io/post/interested-in-new-mixed-mode-research/</link>
      <pubDate>Thu, 07 Mar 2013 15:04:00 +0100</pubDate>
      <guid>https://thomvolker.github.io/post/interested-in-new-mixed-mode-research/</guid>
      <description>&lt;p&gt;Mixed-mode research is still a hot topic among survey methodologists. At least at about every meeting I attend (some selection bias is likely here). Although we know a lot from experiments in the last decade, there is also a lot we don&#39;t know. For example, what designs reduce total survey error most? What is the optimal mix of survey modes when data quality and survey costs are both important? And, how can we compare mixed-mode studies across time, or countries, when the proportions of mode assignments changes over time or vary between countries?&lt;/p&gt;
&lt;p&gt;Together with some researchers at National Statistical Institutes I am trying to form a European consortium to set up a programme for doing comparative mixed-mode research. One of the goals of this consortium would be to apply for funding at the next wave of EU funding (horizon 2020). We are however still looking for researchers in market research, official statistics and universities. And especially from countries in Northern, Central and Southern Europe. Want to know more? Write me at p.lugtig AT uu.nl&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Is panel attrition the same as nonresponse?</title>
      <link>https://thomvolker.github.io/post/is-panel-attrition-same-as-nonresponse/</link>
      <pubDate>Sat, 29 Sep 2012 10:49:00 +0200</pubDate>
      <guid>https://thomvolker.github.io/post/is-panel-attrition-same-as-nonresponse/</guid>
      <description>&lt;p&gt;All of my research is focused on the methods of assembling and analysis of panel survey data. One of the primary problems of panel survey projects is attrition or drop-out. Over the course of a panel survey, many respondents decide to no longer participate.&lt;/p&gt;
&lt;p&gt;Last july I visited the panel survey methods workshop in Melbourne, at which we had extensive discussions about panel attrition. How to study it, what the consequences are (bias) for survey estimates, and how to prevent it from happening altogether.&lt;/p&gt;
&lt;p&gt;These questions have a lot in common with the questions that are being discussed at another workshop for survey methodologists: the nonresponse workshop. The only difference is that at the nonresponse workshop we discuss one-off, cross-sectional surveys, and at the panel survey workshop, we dicuss what happens after the first wave of data collection.&lt;/p&gt;
&lt;p&gt;I am in the middle of writing a book chapter (with Annette Scherpenzeel and Marcel Das of Centerdata) on attrition in the LISS Internet panel, and one of the questions that we try to answers is whether nonrespondents in the first wave are actually similar to respondents who drop out at wave 2 or later. Or to be more precise, whether nonrespondents are actually similar to fast attriters, or to other sub-groups of attriters.&lt;/p&gt;
&lt;p&gt;The graph below shows attrition patterns for the people in the LISS panel for the 50 waves that we analysed. The green line on top represents people who have response propensities close to 1, meaning they always participate. The brown line represents fast attriters, and the pink, dark blue, and purple lines slowers groups that drop out more slowly. You also find new panel entrants (dark grey and red line), and finally, a almost invisible black line that has response propensities of 0, meaning that although these people consented to become a panel member, they never actually participate in the panel.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://2.bp.blogspot.com/-bQM6mKwQHSQ/UGa00GA-Y0I/AAAAAAAACfM/nuzk9xKEmC0/s1600/graph&amp;#43;lca9.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://2.bp.blogspot.com/-bQM6mKwQHSQ/UGa00GA-Y0I/AAAAAAAACfM/nuzk9xKEmC0/s400/graph+lca9.jpg&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;click on the Figure to enlarge&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For the whole story you&#39;ll have to wait for book on &amp;lsquo;Internet panel surveys&amp;rsquo; to come out somewhere in 2013, but I&#39;ll focus here on comparing initial nonrespondents to respondents who do consent, but then never participate.&lt;br&gt;
These groups turn out to be different. Not just a little different, but hugely different. This was somewhat surprising to me, as many survey methodologists believe that early panel attrition is some kind of continuation of initial nonresponse. It turns out not to be. Fast attriters are very different from initial nonrespondents. My hypothesis for this is that some specific groups of people may &#39; accidentily&amp;rsquo;  say yes to a first survey request, but then try to get out of the survey as fast as they can. I am still not sure what this implies for panel research (comments very welcome): does it mean that the methods that we use to target nonrespondents (persuasion principles of Cialdini et al. 1991) might not work in panel surveys, and that we need to use different methods?&lt;/p&gt;
&lt;p&gt;I think the first few waves of a panel study are extremely important for keeping attrition low in the long run. So, I think we should perhaps prolong some of the efforts that we use in the recruitment phase (advance letters, mixed-mode contact strategy), in the first waves as well, only to resort to a cheaper contact mode later, once panel members have developed a habit of responding to the different waves in a panel.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>why access panels cannot weight elections polls accurately</title>
      <link>https://thomvolker.github.io/post/why-access-panels-cannot-weight/</link>
      <pubDate>Sat, 22 Sep 2012 16:05:00 +0200</pubDate>
      <guid>https://thomvolker.github.io/post/why-access-panels-cannot-weight/</guid>
      <description>&lt;p&gt;There are a lot of reasons why would not want to use acces panels for predicting electoral outcomes . These are well discussed in many places on- and offline. I&#39;ll shortly summarize them, before adding some thoughts to why access panels do so badly predicting election outcomes.&lt;/p&gt;
&lt;p&gt;1. Access panels don&#39;t draw random samples, but rely on self-selected samples. A slightly better way to get panel respondents is a quota sample, but even these have problems, well discussed here, here and here for example. The bottom line is that access panel respondents are not &#39; normal&amp;rsquo; people, and so voting preferences of not-normal people are likely to be biased.&lt;/p&gt;
&lt;p&gt;2. Because of these problems, survey managers use weighting. They correct their sample for known biases in the sample. If they know elderly people with low educations are underrepresented in an access panel, they weigh them up. I think this is bad practice. And it has been shown that weighting does not solve the problem,. and can sometimes make biases worse for general surveys. Here are some additional and specific problems, often neglected. In short, weighting only works if the weighting variables can predict the dependent variable to a great extent.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://1.bp.blogspot.com/-vjBRTegZb3Q/UF3EyedoEQI/AAAAAAAACe8/lc7kLoJxNis/s1600/Unbalanced_scales-too-far-right.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://1.bp.blogspot.com/-vjBRTegZb3Q/UF3EyedoEQI/AAAAAAAACe8/lc7kLoJxNis/s200/Unbalanced_scales-too-far-right.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Weighting is usually done with socio-demographic variables. From political science research, we know that sociodemographics do a bad job of explaining voting behavior. Explained variances for regression models normally don&#39;t exceed 10%.&lt;br&gt;
So, let me get down to the main point I would like to make in this post. A point which I have not seen discussed anywhere.&lt;/p&gt;
&lt;p&gt;Panel survey managers have &amp;lsquo;resolved&amp;rsquo; the weakness of their weighting models by including a variable that does predict voring behavior fairly well: past voting behavior. If one knows that past Social Democrat voters are underrepresented, one can weight on that variable. This is all very well, if one has good data of past voting behavior for all panel members. The panels currently do not. Their information is wrong in two ways:&lt;/p&gt;
&lt;p&gt;1. Access panels will never have information for people &lt;strong&gt;who did not vote previously&lt;/strong&gt;. These are mainly young people, or people who normally do not vote in elections. If these new voters vote like everyone else there is no problem, but new voters have very specific voting preferences.&lt;/p&gt;
&lt;p&gt;2.  Reversely, access panels can not predict well &lt;strong&gt;who is not going to vote in current elections&lt;/strong&gt;. If non-voters disproportionally voted for one party in the previous elections, this will lead to an overestimation of voters for that party.&lt;/p&gt;
&lt;p&gt;I believe these two problems are larger than most people think. The first problem can predict why the PVV-vote was underestimated in 2006 and 2010. The PVV attracted many new voters in those elections. The second problem explains why the PVV-vote was overestimated in 2012.  Many people who voted PVV in the previous elections, stayed home this time.&lt;/p&gt;
&lt;p&gt;So, panel survey managers who want a bit of free advice how to improve your polls. Try to get a clear view on the new voters, and the people unlikely to vote. That may be hard, especially because non-voters are not so interested in politics, and will therefore not sign up for online access panels voluntarily. But it is certainly not impossible.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dependent Interviewing, and stability in opinion polls</title>
      <link>https://thomvolker.github.io/post/dependent-interviewing-and-longitudinal/</link>
      <pubDate>Wed, 15 Aug 2012 19:13:00 +0200</pubDate>
      <guid>https://thomvolker.github.io/post/dependent-interviewing-and-longitudinal/</guid>
      <description>&lt;p&gt;I was re-reading one of the papers I wrote as part of my dissertation on survey data quality in panel surveys. The paper deals with the effects of the introduction of an interviewing technique called Dependent Interviewing in the British Household Panel Survey. I wrote this paper together with Annette Jackle, and if you are interested after reading the next bit, you can download a working paper version of it 
&lt;a href=&#34;https://www.iser.essex.ac.uk/publications/working-papers/iser/2011-23.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here.&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Dependent Interviewing uses data from respondents from earlier interviews in survey questions. Instead of asking respondents every year the question &lt;em&gt;&amp;ldquo;what types of income do you receive&lt;/em&gt;&amp;quot;, you can also ask them:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;last year, you told us that you receive income from your private pension plan, the state pension, as well as income from renting out a house. Is this still the same?&amp;quot;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There are of course multiple ways in which you can use information like this, and the BHPS actually uses Dependent Interviewing in a slightly more sophisticated way, but the basic idea is in my opinion quite intuitive. Why would you ask the same questions time and time again, when you already know so much about respondents?&lt;/p&gt;
&lt;p&gt;The paper we wrote documents the effects on data quality, and specifically investigates what the effect of Dependent Interviewing is on measures of household income. In short, the effects are not huge, but it turns out that Dependent Interviewing is especially effective for poorer households. These households depend for a large part on different kinds of government transfers, and these are easily forgotten or underreported. When the effects of Dependent Interviewing are taken into account, the poorer households become a little richer, and so, all in all, poverty is actually a little lower than was previously estimated.&lt;/p&gt;
&lt;p&gt;Perhaps interesting to Dutch readers, one of the main pollers in the Netherlands, Maurice de Hond, is also using Dependent Interviewing in his surveys (on all questions!). I am a member of his panel, and when I complete a survey, I only have to change answers if I want to, and otherwise just confirm my answers from the previous waves.&lt;/p&gt;
&lt;p&gt;I see why Maurice de Hond has chosen to do this. Electoral preferences are very volatile, and panel surveys on voter preferences are perhaps too volatile. But I have serious doubts whether Dependent Interviewing here solves volatility. It rather creates articficial stability. In the first week of july, Maurice de Hond polled an average weekly change of seats of 10. Ipsos Synovate (see my earlier posts on why I trust them most), 12. Actually a small difference. There are many newspapers following and criticising the actual poll results. I&#39;ll try to keep you updated on volatility across the polls, meanwhile trying to answer the question whether one should trust stable polls, or volatile polls.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>how to use Internet panels for polling</title>
      <link>https://thomvolker.github.io/post/how-to-use-internet-panels-for-polling/</link>
      <pubDate>Thu, 10 Feb 2011 11:13:00 +0100</pubDate>
      <guid>https://thomvolker.github.io/post/how-to-use-internet-panels-for-polling/</guid>
      <description>&lt;p&gt;Before people believe I&#39;m old-fashioned, I do think that Internet-surveys, even panel surveys are the future of survey research. John Krosnick makes some good points in a video shot by the people from 
&lt;a href=&#34;http://www.blogger.com/www.pollster.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.pollster.com&lt;/a&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>checklist for quality of opinion polls</title>
      <link>https://thomvolker.github.io/post/checklist-for-quality-of-opinion-polls/</link>
      <pubDate>Thu, 10 Feb 2011 10:38:00 +0100</pubDate>
      <guid>https://thomvolker.github.io/post/checklist-for-quality-of-opinion-polls/</guid>
      <description>&lt;p&gt;1. Is it clear who ordered and financed the poll?&lt;br&gt;
2. Is there a report documenting the poll&#39;s procedures?&lt;br&gt;
3. Is the target population clearly described?&lt;br&gt;
4. is the questionnaire available and has it been tested?&lt;br&gt;
5. what were the sampling procedures?&lt;br&gt;
* the sample should be drawn for the target population. If it only contains for example people with Internet access, be careful&lt;br&gt;
6. What is the number of respondents?&lt;br&gt;
7. Is the response percentage sufficient?&lt;br&gt;
* it is difficult to say what percentage is sufficient. Higher response percentages do not automatically lead to better data quality. 10 or 20 % is however too low.&lt;br&gt;
8. Have the data been weighted?&lt;br&gt;
9. Are the margins of error being reported?&lt;/p&gt;
&lt;p&gt;For more info, check the website of 
&lt;a href=&#34;http://www.blogger.com/www.survey-onderzoek.nl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jelke Bethlehem&lt;/a&gt;
 (in Dutch), or download the checklist 
&lt;a href=&#34;http://www.survey-onderzoek.nl/papers/Checklist-voor-Peilingen-V2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
, with 
&lt;a href=&#34;http://www.survey-onderzoek.nl/papers/Checklist-voor-Peilingen-Uitleg-V2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;an explanation&lt;/a&gt;
 (in Dutch)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>the influence of polls on voting behavior</title>
      <link>https://thomvolker.github.io/post/influence-of-polls-on-voting-behavior/</link>
      <pubDate>Thu, 10 Feb 2011 10:11:00 +0100</pubDate>
      <guid>https://thomvolker.github.io/post/influence-of-polls-on-voting-behavior/</guid>
      <description>&lt;p&gt;Many opinion pollers do badly when it comes to predicting elections. This is mainly because they let their respondents self-select them into their polls. So what, who cares? The polls make for some good entertainment and easily fill the talk-shows on television. If everyone knows they cannot be trusted, why care?&lt;/p&gt;
&lt;p&gt;We should care. In the Dutch electoral system - with 
&lt;a href=&#34;http://en.wikipedia.org/wiki/Proportional_representation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;poportional respresentation&lt;/a&gt;
 - every vote counts. If only a small percentage of voters lets their vote depend on the polls of the election result, this can result in shifts of several seats in parliament. It is unclear how many voters decide how to vote based on the opinion polls, but it is a fact that there are many voters who consider voting for two or more parties, and many who do vote strategically. The 
&lt;a href=&#34;http://www.icpsr.umich.edu/icpsrweb/ICPSR/series/25&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dutch Parliamentary Election Study&lt;/a&gt;
 (DPES) in 2006 found that 18% of voters indicated that they let their vote be influenced by the election polls. This amounts to a total of 27 parliamentary seats: almost the number of seats of the largest party in the 
&lt;a href=&#34;http://www.tweedekamer.nl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;current parliament&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;As long as voters choose strategically in different ways this may not matter. If someone votes strategically to make sure a new government has the the greens in it, but someone else votes strategically for labour to make sure his or her favourite candidate becomes prime mininster, the net effect of strategical voting might be zero or very small. There is evidence however, that this is not the case. People like to vote for winners. This is called the 
&lt;a href=&#34;http://en.wikipedia.org/wiki/Bandwagon_effect&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bandwagon effect&lt;/a&gt;
. Whenever labour does well in the (biased) opinion polls, more voters will consider voting for them. This may in the end lead to the fact political parties (and pollers) have a lot of interest to do well in polls. In fact, it may be tempting to publish fraudulent polls. This seems to be increasingly common in the United States, where they call them 
&lt;a href=&#34;http://www.aapor.org/AAPOR_Statements_on_Push_POlls.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;push polls&amp;rdquo;&lt;/a&gt;
 . Publish fraudulent polls on purpose to make public opinion shift in your favor.&lt;/p&gt;
&lt;p&gt;So, what to do about it? First, I think it would be fair not to publish any opinion polls some time before election day, as is done in 
&lt;a href=&#34;http://www.scribd.com/doc/259320/Regulating-Election-Polls&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;France for example&lt;/a&gt;
 (albeit only for two days). Second, journalists and newsreaders should be very critical towards opinion polls, and only publish them when some basic quality criteria have been assessed and met. The 
&lt;a href=&#34;http://www.blogger.com/www.npso.net&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dutch Organisation on Survey Research&lt;/a&gt;
 has taken the initiative to develop a checklist for journalists. I will put it online soon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>predicting elections</title>
      <link>https://thomvolker.github.io/post/predicting-elections/</link>
      <pubDate>Mon, 17 Jan 2011 17:45:00 +0100</pubDate>
      <guid>https://thomvolker.github.io/post/predicting-elections/</guid>
      <description>&lt;p&gt;Opinion pollers do a lousy job of predicting elections. For a good read, see for example the prediction of the 
&lt;a href=&#34;http://en.wikipedia.org/wiki/New_Hampshire_Democratic_primary,_2008&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;New Hampshere primary&lt;/a&gt;
 in 2008, when all polls predicted Obama to win, but it was Clinton who won (albeit by a slim margin).&lt;/p&gt;
&lt;p&gt;In the Dutch context, there are three main polling firms, that each do equally well (or badly). Out of a hundred and fifty parliamentary seats, 
&lt;a href=&#34;http://www.blogger.com/www.peil.nl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;peil.nl&lt;/a&gt;
 mispredicted 20, while 
&lt;a href=&#34;http://www.tns-nipo.com/pages/nieuws-pers-politiek-tk2010.asp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TNS-NIPO&lt;/a&gt;
 and 
&lt;a href=&#34;http://www.synovate.nl/content.asp?targetid=621&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synovate&lt;/a&gt;
 shared the honor of only missing the target by 16 seats in the 2010 parliamentary election. These polls were conducted the day before the election, and some of the pollers said that people might have changed their vote at the last minute. That may very well be, but even the exit poll on the night of the election was wrong. 
&lt;a href=&#34;http://en.wikipedia.org/wiki/Dutch_general_election,_2010&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Peil.nl was 17 seats off&lt;/a&gt;
 and 
&lt;a href=&#34;http://en.wikipedia.org/wiki/Dutch_general_election,_2010&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TNS NIPO 15&lt;/a&gt;
. 
&lt;a href=&#34;http://en.wikipedia.org/wiki/Dutch_general_election,_2010&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Only Synovate&lt;/a&gt;
 did a lot better, and only missed the true result by 3 seats. I will discuss why this is in a next post, but it is just a matter of speed and low costs versus quality.&lt;/p&gt;
&lt;p&gt;And we have known for a long, long time how to do exit polls. Although there was public outcry in the UK, when the exit poll 
&lt;a href=&#34;http://news.bbc.co.uk/2/hi/uk_news/politics/election_2010/8666266.stm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;predicted the liberal democrats not to win the elections&lt;/a&gt;
, it was spot on. If we know how to do it, then why don&#39;t we?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>first past the post</title>
      <link>https://thomvolker.github.io/post/first-past-post/</link>
      <pubDate>Mon, 10 Jan 2011 17:14:00 +0100</pubDate>
      <guid>https://thomvolker.github.io/post/first-past-post/</guid>
      <description>&lt;p&gt;Dear all,&lt;/p&gt;
&lt;p&gt;With a new year come new year&#39;s resolutions. I have been working as a survey methodologist for about the last 5 years. I teach and I do research. Teaching gives instant rewards, or at least instant feedback. I like that. Doing research is however a different matter. It is a slow and sometimes agonizing process of muddling through (for me).&lt;/p&gt;
&lt;p&gt;Studies remain in review forever, sometimes don&#39;t make it at all into a publication, while some of my ideas or views just never make onto paper at all. I hope this blog fills that gap.&lt;/p&gt;
&lt;p&gt;I will write in English, but might occasionally do so in Dutch if I feel like it. As far as content goes, I&#39;m not sure where all of this will lead. I might post very academic-like things very frequently, but could also publish every once in a while.&lt;/p&gt;
&lt;p&gt;As a survey methodologist my view is that data matter. Policy makers, and academics use data too often without really knowing how the data were gathered, and whether they are trustworthy. Over the past five years it is my experience that data quality is often low, leading to badly informed or even wrong decisions. Data quality is far more important that fancy statistical models or cool graphs. Hopefully you will enjoy my adventures in the jungle of improving survey data quality.&lt;/p&gt;
&lt;p&gt;Your singalong survey methodologist,&lt;/p&gt;
&lt;p&gt;Peter&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
