<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>questionnaire design | Peter Lugtig</title>
    <link>https://thomvolker2.github.io/tags/questionnaire-design/</link>
      <atom:link href="https://thomvolker2.github.io/tags/questionnaire-design/index.xml" rel="self" type="application/rss+xml" />
    <description>questionnaire design</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019 - 2020</copyright><lastBuildDate>Thu, 27 Apr 2017 19:52:00 +0200</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>questionnaire design</title>
      <link>https://thomvolker2.github.io/tags/questionnaire-design/</link>
    </image>
    
    <item>
      <title>Mobile-only web survey respondents</title>
      <link>https://thomvolker2.github.io/post/mobile-only-web-survey-respondents/</link>
      <pubDate>Thu, 27 Apr 2017 19:52:00 +0200</pubDate>
      <guid>https://thomvolker2.github.io/post/mobile-only-web-survey-respondents/</guid>
      <description>&lt;p&gt;My breaks between posts are getting longer and longer. Sorry my dear readers. Today, I am writing about research done over a year ago that I did with 
&lt;a href=&#34;https://www.uu.nl/staff/VToepoel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vera Toepoel&lt;/a&gt;
 and 
&lt;a href=&#34;https://www.linkedin.com/in/alerk-amin-908394/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alerk Amin.&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Our study was about a group of respondents we can no longer ignore: Mobile-only web survey respondents. These are people, who do no longer use a laptop or desktop PC and use their smartphone for most or any of their Internet browsing, but instead use a smartphone. If we as survey methodologists want these people to answer our surveys, we &lt;em&gt;have to&lt;/em&gt; design our surveys for smartphones as well.&lt;/p&gt;
&lt;p&gt;Who are these mobile-only web survey respondents? This population may of course differ across countries. We used data from the American Life Panel, run by RAND, to investigate what this group looked like in the United States, using data from 2014 (so the situation today may be a bit different). The full paper can be found 
&lt;a href=&#34;https://surveypractice.scholasticahq.com/article/2803-mobile-only-web-survey-respondents&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;We find that of all people participating in 7 surveys conducted in the panel, 7% is mobile-only in practice. This is not a huge proportion, but it may matter a lot if these 7% of respondents are very different from other types of respondents. We find that they are.&lt;/p&gt;
&lt;p&gt;In order to study how different these respondents are, we define 5 groups based on the device the use for responding to surveys:&lt;br&gt;
1. Respondents who always use a PC for completing surveys. This the largest group (68%) and therefore serves as the reference group)&lt;br&gt;
2. Respondents who always use a tablet (5%)&lt;br&gt;
3. Respondents who always use a smartphone ( 7% - our group of interest)&lt;br&gt;
4. Respondents who mix tablets and Pcs (7%)&lt;br&gt;
5. Respondents who mix phones and Pcs (10%)&lt;br&gt;
A further 1% uses all devices, but we ignore these respondents here.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://2.bp.blogspot.com/-tsKhJYMzMnQ/WQIezJ94h6I/AAAAAAAAC6c/DqazrgtOwMkPZjqYfnvo2By_Rhay2tPFwCLcB/s1600/plot%2Bmarginal%2Beffcets.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://2.bp.blogspot.com/-tsKhJYMzMnQ/WQIezJ94h6I/AAAAAAAAC6c/DqazrgtOwMkPZjqYfnvo2By_Rhay2tPFwCLcB/s400/plot%2Bmarginal%2Beffcets.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Click Figure to enlarge. The effects shown are always in comparison to the reference group, which is the ‘always PC’ group.&lt;/p&gt;
&lt;p&gt;The 5 groups serve as our dependent variable in a multinomial logit regression. The average marginal effects shown in the Figure above respresent the change in the likelihood of being in the &amp;lsquo;always PC&amp;rsquo; group as compared to one of the other groups. The negative age coefficient of -.03 for the always phone group means that with every decade respondents get younger (a negative effect), they have a .03 higher probability to be be in the always phone group as referred to the always Pc group. These effects seem small, but they are not. An imaginary respondent aged 60 has a predicted probability of 92 percent of being in the always PC group as opposed to the always phone group, but this probability is about 80 percent for someone aged 20, controlling for the effects of other covariates.&lt;/p&gt;
&lt;p&gt;Our take-away? Apart from age, &amp;lsquo;Always phone’ respondents are also less likely to have a higher education (Bachelor degree or higher), are more likely to be married, and more likely to be of Hispanic or African American ethnicity. These characteristics coincide with some of the most important characteristics of hard-to-recruit respondents. While designing your surveys for smartphones will not get these hard-to-recruit respondents into your panel, you can easily lose them by not designing your surveys for smartphones.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dependent Interviewing and the risk of correlated measurement errors</title>
      <link>https://thomvolker2.github.io/post/dependent-interviewing-and-risk-of/</link>
      <pubDate>Sat, 03 Aug 2013 16:22:00 +0200</pubDate>
      <guid>https://thomvolker2.github.io/post/dependent-interviewing-and-risk-of/</guid>
      <description>&lt;p&gt;Longitudinal surveys ask the same people the same questions over time. So questionnaires tend to be rather boring for respondents after a while. &amp;ldquo;Are you asking me this again, you asked that last year as well!&amp;rdquo; is what many respondents probably think during an interview. As methodologists who manage panel surveys, we know this process may be rather boring, but in order to document change over time, we just need to ask respondents the same questions over and over.&lt;/p&gt;
&lt;p&gt;Some measures of change over time would become biased if we just repeat questions year-on-year. For example, we know that if we ask respondents twice about their occupation, less than half of all of them have the same occupational codes over time. We know from other statistics (e.g. tax returns), that that is not true. Most people stay in the same occupation over time. Now, you may think, dear reader, that that is probably due to the fact that occupation is rather difficult to measure and code in general, and you are right. Unreliable question will lead to a lot of spurious change over time.&lt;/p&gt;
&lt;p&gt;Dependent Interviewing helps to make codes consistent over time and reduce such spurious change. The idea is that, instead of coding occupation independently year-on-year, you ask respondents in year 2 the question &amp;ldquo;last year, you said you were a bankteller, is that still the case?&amp;quot;. There are many different variants to ask this Dependent Interviewing question, and the exact wording is important for the outcomes. Especially, because we do not want respondent to say &amp;ldquo;yes&amp;rdquo; too easily to questions we ask.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://1.bp.blogspot.com/-Nn9LXqCRaXc/Uf0PNO7KjdI/AAAAAAAACmM/-cLYJqtwcjw/s1600/interview.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://1.bp.blogspot.com/-Nn9LXqCRaXc/Uf0PNO7KjdI/AAAAAAAACmM/-cLYJqtwcjw/s1600/interview.jpg&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&amp;ldquo;Last year, you told me you told me you worked as a bankteller, is that still the case?&amp;quot;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Recently, a paper I wrote on the effects of various forms of Dependent Interviewing came out in 
&lt;a href=&#34;https://dl.dropboxusercontent.com/u/2839696/Lugtig%20and%20Lensvelt-Mulders%20%28preprint%29%20evaluating%20the%20effect%20of%20DI%20on%20the%20quality%20of%20measures%20of%20change.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Field Methods&lt;/a&gt;
. It was actually the first paper I wrote for my Ph.D, and I started work on it in 2006. So, it has been quite a journey to get this story on paper and get it published. I am very happy to see it on paper now. We did an experiment, where we tried out different DI-designs in a four-wave panel study, to study effects of data quality of each different DI-design. Specifically, we looked at whether respondents might falsely confirm data from the previous year that we knew contained measurement error. The bottom line of the study is that when Dependent Interviewing is applied to income amount questions over time, it does improve data quality, and we don&#39;t need to worry so much about respondents wrongly agreeing to pre-loaded data from the  previous year. Read the full paper 
&lt;a href=&#34;https://dl.dropboxusercontent.com/u/2839696/Lugtig%20and%20Lensvelt-Mulders%20%28preprint%29%20evaluating%20the%20effect%20of%20DI%20on%20the%20quality%20of%20measures%20of%20change.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here.&lt;/a&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Designing mixed-mode surveys</title>
      <link>https://thomvolker2.github.io/post/designing-mixed-mode-surveys/</link>
      <pubDate>Sat, 12 Jan 2013 10:24:00 +0100</pubDate>
      <guid>https://thomvolker2.github.io/post/designing-mixed-mode-surveys/</guid>
      <description>&lt;p&gt;This weekend is the deadline for submitting a presentation proposal to this year&#39;s conference of the European Survey Research Association. That&#39;s one the two major the conferences for people who love to talk about things like nonresponse bias, total survey error, and mixing survey modes.&lt;/p&gt;
&lt;p&gt;As in previous years, it looks like the most heated debates will be on mixed-mode surveys. As survey methodologists we have been struggling to combine multiple survey modes (Internet, telephone, face-to-face, mail) in a good way. And we no longer can rely on just one survey mode to do a good survey. Many people don&#39;t have Internet, or landline phones, and face-to-face modes are becoming too expensive.&lt;/p&gt;
&lt;p&gt;When we start mixing survey modes, we run into all kinds of problems. Different people respond in different survey modes, leading to differences in the variables we&#39;re interested in (selection effects). And we also know that people respond differently to the same survey question when asked in two different modes (measurement or &amp;lsquo;mode&amp;rsquo;  effect).&lt;/p&gt;
&lt;p&gt;We have been trying to assess the size of  selection and mode effects in mixed-mode surveys for a long time. I have written about this issue earlier, and although we have made progress, I don&#39;t think we will ever be able to &amp;lsquo;correct&amp;rsquo; for differences between survey modes. That is because the sizes of selection and mode effects will always differ from survey to survey, depending on the topic of the study, and the population studied.&lt;/p&gt;
&lt;p&gt;So, I suggest we try something different. We don&#39;t we try to make survey modes themselves similar? If you think of combining the Internet, with face-to-face (two very different modes), these could be some ways to make the surveys equivalent.&lt;/p&gt;
&lt;p&gt;- Approach people in the same way. For example, use advance letters in both modes, and then call respondents to either make an appointment (Face-to-face), or ask for an e-mailaddress and do an Internet interview. Stupid it may sound, but mixed-mode surveys or experiments often change the entire protocol, so we have no idea what is caused by what.&lt;br&gt;
- Use showcards in a face-to-face survey for sensitive questions and let the respondent self-complete them, like they do on the Internet&lt;br&gt;
- Use a Virtual Reality interviewer on an Internet survey for difficult questions, to help the respondent answering questions.&lt;br&gt;
- Use an audio recording feature on the Internet, so respondents can give answers in the same way as they do in Face-to-face surveys&lt;br&gt;
- Use short and simple questions that can easily be asked in any survey mode in more or less the same manner. This implies minimizing the number of response categories to only a few for any question.&lt;/p&gt;
&lt;p&gt;Surely, this approach is not fault free, and I do see lots of practical issues. I do think however that this is a better way to move forward than just trying fix up the mess with statistics after the data collection. These correction methods will always be necessary, but relying on them too much puts too much faith in statistics.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
