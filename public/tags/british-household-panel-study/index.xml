<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>British Household Panel Study | Peter Lugtig</title>
    <link>https://peterlugtig.github.io/tags/british-household-panel-study/</link>
      <atom:link href="https://peterlugtig.github.io/tags/british-household-panel-study/index.xml" rel="self" type="application/rss+xml" />
    <description>British Household Panel Study</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019 - 2020</copyright><lastBuildDate>Tue, 29 Apr 2014 14:08:00 +0200</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>British Household Panel Study</title>
      <link>https://peterlugtig.github.io/tags/british-household-panel-study/</link>
    </image>
    
    <item>
      <title>Are item-missings related to later attrition?</title>
      <link>https://peterlugtig.github.io/post/are-item-missings-related-to-later/</link>
      <pubDate>Tue, 29 Apr 2014 14:08:00 +0200</pubDate>
      <guid>https://peterlugtig.github.io/post/are-item-missings-related-to-later/</guid>
      <description>&lt;p&gt;A follow up on 
&lt;a href=&#34;http://www.peterlugtig.com/2014/03/do-respondents-become-sloppy-before.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;last month&amp;rsquo;s post&lt;/a&gt;
. Respondents do seem to be less compliant in the waves before they drop out from a panel survey. This may however not neccesarily lead to worse data. So, what else do we see before attrition takes place? Let have a look at missing data:&lt;/p&gt;
&lt;p&gt;First, we look at missing data in a sensitive question on income amounts. Earlier studies (
&lt;a href=&#34;http://www.jos.nu/Articles/abstract.asp?article=260145&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
, 
&lt;a href=&#34;http://www.jstor.org.proxy.library.uu.nl/stable/146438&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here,&lt;/a&gt;
 
&lt;a href=&#34;http://www.jstor.org.proxy.library.uu.nl/stable/1392158&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
) have already found that item nonresponse on sensitive questions predicts later attrition. I find that item nonresponse does increase before attrition, but only because of the fact that respondents are more likely to refuse to give an answer. And that increase is largely due to respondents who will later refuse to participate in the study as a whole. So, &lt;em&gt;item&lt;/em&gt; refusals are a good predictor of later &lt;em&gt;study&lt;/em&gt; refusals. The proportion of &amp;ldquo;Don&amp;rsquo;t know&amp;rdquo; respondents does not increase over time.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://1.bp.blogspot.com/-ZLkf9j9-qUk/U1-RxvTaZTI/AAAAAAAACqc/OqCXDLSAX1s/s1600/missings&amp;#43;PAYGL.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://1.bp.blogspot.com/-ZLkf9j9-qUk/U1-RxvTaZTI/AAAAAAAACqc/OqCXDLSAX1s/s1600/missings+PAYGL.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Missing income data in BHPS in 5 waves before attrition (click to enlarge)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Does this finding for a sensitive question extend to all survey questions? No. Over all questions combined, I find that refusals  increase before attrition takes place, but  from a very low base (see the Y-axis scale in the figure below). Moreover, there is no difference between the groups, meaning that those who drop out of the survey do not have more item-missings than those respondents who are &amp;ldquo;always interviewed&amp;rdquo;. It may seem odd that item missings increase for respondents who always happily participate. I suspect however that this may be related to the fact that both interviewers and respondents may have known in the last wave(s) 
&lt;a href=&#34;https://www.iser.essex.ac.uk/bhps&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;that the BHPS was coming to an end after 18 years of interviewing.&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://4.bp.blogspot.com/-S-ht4QK3lzQ/U1-TBeejkWI/AAAAAAAACqo/suuoODyJAik/s1600/dkplot.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;br&gt;
&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://2.bp.blogspot.com/-LkHjq_AVszE/U1-TBdKdzbI/AAAAAAAACqk/0HLk_2un_0c/s1600/refuseplot.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://2.bp.blogspot.com/-LkHjq_AVszE/U1-TBdKdzbI/AAAAAAAACqk/0HLk_2un_0c/s1600/refuseplot.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Missing data for all survey questions in BHPS in waves before attrition (click to enlarge)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;What to do with this information? It seems that later study refusals can be identified using a combination of item nonresponses and survey compliance indicators. Once these respondents are identified, the next step would be to target them with survey design features that try to prevent attrition. These survey design features should target some of the concerns and motivations such respondents have that cause them to drop out from the survey.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Do respondents become sloppy before attrition?</title>
      <link>https://peterlugtig.github.io/post/do-respondents-become-sloppy-before/</link>
      <pubDate>Fri, 28 Mar 2014 15:33:00 +0100</pubDate>
      <guid>https://peterlugtig.github.io/post/do-respondents-become-sloppy-before/</guid>
      <description>&lt;p&gt;I am working on a paper that aims to link measurement errors to attrition error in a panel survey. For this, I am using the British Household Panel Survey. 
&lt;a href=&#34;http://www.peterlugtig.com/2013/11/longitudinal-interview-outcome-data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;In an earlier post&lt;/a&gt;
 I already argued that attrition can occur for many reasons, which I summarized in 5 categories.&lt;/p&gt;
&lt;p&gt;1. Noncontact&lt;br&gt;
2. Refusal&lt;br&gt;
3. Inability (due to old age, infirmity) as judged by the interviewer, also called &amp;lsquo;other non-interview&amp;rsquo;.&lt;br&gt;
4. Ineligibibility (due to death, or move into institution or abroad).&lt;br&gt;
5. people who were always interviewed&lt;/p&gt;
&lt;p&gt;In the paper, I study whether attrition due to any of the reasons above can be linked to increased measurement errors in the last waves before attrition. For example, earlier studies have found that item nonresponse to sensitive questions (income) predicts unit nonresponse in the next waves.&lt;/p&gt;
&lt;p&gt;For every respondent in the BHPS, I coded different indicators measurement error in every of the last five waves before attrition takes place. My working hypothesis is that measurement errors should increase in the last few waves before attrition takes place, due to decreasing respondent willingness and/or capability to participate.&lt;/p&gt;
&lt;p&gt;In the figure below, you find one set of indicators I used. Compliance to the survey does not count as an indicator of measurement error, but I found it interesting to look into nonetheless. I find that respondents are far less keen to do &amp;ldquo;extra&amp;rdquo; tasks in the waves before attrition. As measures, of compliance to these extra tasks, I looked at:&lt;/p&gt;
&lt;p&gt;1. the respondent cooperation as judged by the interviewer.&lt;br&gt;
2 the proportion of respondents who completes the tracking schedule at the end of the interview, and&lt;br&gt;
3. the proportion of respondents returning a self-completion questionnaire, left after the interview.&lt;/p&gt;
&lt;p&gt;In order to be able to interpret the results in a good way, I contrasted the 4 attrition groups with the 5th group of respondents who do not drop out, and are always interviewed.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://2.bp.blogspot.com/-L56iVULfRtk/UzWBGVFLSaI/AAAAAAAACqA/ceaAjOfnVfM/s1600/compliance.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;http://2.bp.blogspot.com/-L56iVULfRtk/UzWBGVFLSaI/AAAAAAAACqA/ceaAjOfnVfM/s1600/compliance.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Compliance with survey task by respondents in last 5 waves before attrition (click to enlarge)&lt;/p&gt;
&lt;p&gt;Unsuprisingly, I find that compliance decreases before attrition. Even at 5 waves before attrition, I find differences between the groups, with the &amp;ldquo;always interviewed&amp;rdquo; being most compliant, and the later to &amp;ldquo;refuse&amp;rdquo; group least compliant. The differences between the groups increase, the closer they get to attrition. Of the groups that attrite, the &amp;ldquo;noncontacts&amp;rdquo; and later &amp;ldquo;ineligibles&amp;rdquo; do only a little worse than the &amp;ldquo;always interviewed&amp;rdquo;. The &amp;ldquo;refusers&amp;rdquo; and &amp;ldquo;inables&amp;rdquo; have sharply decreasing cooperation ratings, and rates of completing the tracking schedule and returning the self-completion questionnaire. The differences between the groups are not large enough to predict exactly who is going to refuse or become unable to participate, but they can help to identify respondents being at risk.&lt;/p&gt;
&lt;p&gt;The next question would be what to do with this knowledge.  If a respondent really is unable to participate, there is not so much we as survey practitioners can do about this. Likely refusers may also be hard to target effectively. The rate of noncontacts is to a large degree under the control of survey practitioners, and for that reason, many nonresponse researchers are trying to limit noncontacts. Although refusers may be harder to target than noncontacts, it may be easier to identify &lt;em&gt;potential&lt;/em&gt; refusers, and take pre-emptive action, rather than use refusal conversion techniques after a  respondent has refused.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
